{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ff5498-3723-4b73-b5d0-ed10b8766107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7203a4b8-ef8f-4694-93a3-2542e5c5b036",
   "metadata": {},
   "source": [
    "## CYP1A2-CHEMBL1741322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7b7a49-9382-49d3-9b61-aaf4056c8515",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'CYP1A2-CHEMBL1741322'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf6b193-018d-4062-b44c-8c29265439f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_perf = utils_benchmark.load_model_perf(task_name)\n",
    "len(model_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57359402-2bd9-40f8-8bcb-3ef33ece4b28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of compounds: 9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KNN: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [02:16<00:00, 27.20s/it]\n",
      "RF+MACCS: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.85it/s]\n",
      "RF+PubChemFP: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.78s/it]\n",
      "RF+ECFP2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:13<00:00,  2.70s/it]\n",
      "RF+ECFP4: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:50<00:00, 10.05s/it]\n",
      "RF+Daylight: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:55<00:00, 11.16s/it]\n",
      "RF+RDKitFP: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:56<00:00, 11.31s/it]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA L40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 600    | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "---------------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n",
      "21        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a86d9ee11a4339ab7e793b8d4a1270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce83de8a316a4a87973fd5b25fd6b53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 600    | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "---------------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n",
      "21        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a66044df56741c093f0e25771e67f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643109cbae33406ab52bd9131cf4028d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 600    | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "---------------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n",
      "21        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf656d0b79c74307a7e244f35a64006b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb47b06e8a14cbb9ddcd485ef285323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 600    | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "---------------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n",
      "21        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd140621f1a94de69e9159af890e28c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f69f66457247fd942085902c5c560e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K  | train\n",
      "1 | agg             | MeanAggregation    | 0      | train\n",
      "2 | bn              | BatchNorm1d        | 600    | train\n",
      "3 | predictor       | RegressionFFN      | 90.6 K | train\n",
      "4 | X_d_transform   | Identity           | 0      | train\n",
      "---------------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n",
      "21        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee4e6892696409e90a80fb094119115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e17b19071e45df9d16fb970552c72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda:0\n",
      "Loaded pre-trained model with success.\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/morgan-bert/model_molclr.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(checkpoints_folder, 'model.pth'), map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 26.536945343017578\n",
      "0 50 0.43496930599212646\n",
      "0 100 0.32564425468444824\n",
      "0 150 0.3669683635234833\n",
      "0 200 0.31310954689979553\n",
      "Validation loss: 0.4167697773211532 RMSE: 0.64557713\n",
      "1 46 0.6274087429046631\n",
      "1 96 0.2145753800868988\n",
      "1 146 0.20779791474342346\n",
      "1 196 0.34539684653282166\n",
      "Validation loss: 0.40378457431991893 RMSE: 0.63544047\n",
      "2 42 0.3030962646007538\n",
      "2 92 0.2417035847902298\n",
      "2 142 0.3560195565223694\n",
      "2 192 0.4369698166847229\n",
      "Validation loss: 0.4232108315659894 RMSE: 0.65054655\n",
      "3 38 0.19255438446998596\n",
      "3 88 0.3464823365211487\n",
      "3 138 0.2999921441078186\n",
      "3 188 0.42860275506973267\n",
      "Validation loss: 0.4980831452541881 RMSE: 0.70575005\n",
      "4 34 0.375863254070282\n",
      "4 84 0.2098284363746643\n",
      "4 134 0.5671488642692566\n",
      "4 184 0.4043327569961548\n",
      "Validation loss: 0.39913077569670147 RMSE: 0.631768\n",
      "5 30 0.2808693051338196\n",
      "5 80 0.7272630333900452\n",
      "5 130 0.2515257000923157\n",
      "5 180 0.4345325529575348\n",
      "Validation loss: 0.40000037310851944 RMSE: 0.6324558\n",
      "6 26 0.34298521280288696\n",
      "6 76 0.30296263098716736\n",
      "6 126 0.512076735496521\n",
      "6 176 0.1608423888683319\n",
      "Validation loss: 0.4419027715921402 RMSE: 0.66475767\n",
      "7 22 0.19706466794013977\n",
      "7 72 0.23455703258514404\n",
      "7 122 0.5496819019317627\n",
      "7 172 0.2611200213432312\n",
      "Validation loss: 0.456421188182301 RMSE: 0.6755895\n",
      "8 18 0.36851686239242554\n",
      "8 68 0.45291072130203247\n",
      "8 118 0.6480346918106079\n",
      "8 168 0.3654807209968567\n",
      "Validation loss: 0.39250486882196534 RMSE: 0.6265021\n",
      "9 14 0.31494665145874023\n",
      "9 64 0.26058024168014526\n",
      "9 114 0.33952727913856506\n",
      "9 164 0.28462719917297363\n",
      "Validation loss: 0.38163439225819373 RMSE: 0.61776567\n",
      "10 10 0.28649094700813293\n",
      "10 60 0.20381072163581848\n",
      "10 110 0.13344205915927887\n",
      "10 160 0.31769129633903503\n",
      "Validation loss: 0.3951246701180935 RMSE: 0.62858945\n",
      "11 6 0.3584679365158081\n",
      "11 56 0.33136147260665894\n",
      "11 106 0.3289625644683838\n",
      "11 156 0.4227621555328369\n",
      "Validation loss: 0.3816084617541896 RMSE: 0.6177446\n",
      "12 2 0.27614206075668335\n",
      "12 52 0.1999291330575943\n",
      "12 102 0.4305124878883362\n",
      "12 152 0.27026355266571045\n",
      "12 202 0.267708420753479\n",
      "Validation loss: 0.4087138995528221 RMSE: 0.6393073\n",
      "13 48 0.21587789058685303\n",
      "13 98 0.24868130683898926\n",
      "13 148 0.2501751482486725\n",
      "13 198 0.33512237668037415\n",
      "Validation loss: 0.4248889560500781 RMSE: 0.6518351\n",
      "14 44 0.15710300207138062\n",
      "14 94 0.485171914100647\n",
      "14 144 0.36920738220214844\n",
      "14 194 0.2739168703556061\n",
      "Validation loss: 0.40138088208105827 RMSE: 0.6335463\n",
      "15 40 0.28760528564453125\n",
      "15 90 0.36276206374168396\n",
      "15 140 0.3113943040370941\n",
      "15 190 0.25072795152664185\n",
      "Validation loss: 0.37994153507881695 RMSE: 0.616394\n",
      "16 36 0.3022562563419342\n",
      "16 86 0.20439806580543518\n",
      "16 136 0.39483678340911865\n",
      "16 186 0.16193237900733948\n",
      "Validation loss: 0.4981216672394011 RMSE: 0.7057773\n",
      "17 32 0.2652130722999573\n",
      "17 82 0.2584569454193115\n",
      "17 132 0.2625788152217865\n",
      "17 182 0.13778594136238098\n",
      "Validation loss: 0.3871143087744713 RMSE: 0.6221851\n",
      "18 28 0.3733952045440674\n",
      "18 78 0.3106352388858795\n",
      "18 128 0.38968783617019653\n",
      "18 178 0.3972240388393402\n",
      "Validation loss: 0.393436992333995 RMSE: 0.62724555\n",
      "19 24 0.2207297533750534\n",
      "19 74 0.35651448369026184\n",
      "19 124 0.6950951814651489\n",
      "19 174 0.40738433599472046\n",
      "Validation loss: 0.3844485899640454 RMSE: 0.62003917\n",
      "20 20 0.2835952341556549\n",
      "20 70 0.20798641443252563\n",
      "20 120 0.29215848445892334\n",
      "20 170 0.19975119829177856\n",
      "Validation loss: 0.37945082866483265 RMSE: 0.6159958\n",
      "21 16 0.2768656015396118\n",
      "21 66 0.3292352259159088\n",
      "21 116 0.40833431482315063\n",
      "21 166 0.437202513217926\n",
      "Validation loss: 0.37047703315814334 RMSE: 0.60866827\n",
      "22 12 0.1530923843383789\n",
      "22 62 0.30736660957336426\n",
      "22 112 0.24039947986602783\n",
      "22 162 0.39277949929237366\n",
      "Validation loss: 0.3814445638822185 RMSE: 0.617612\n",
      "23 8 0.4410110116004944\n",
      "23 58 0.14790423214435577\n",
      "23 108 0.19146767258644104\n",
      "23 158 0.24228878319263458\n",
      "Validation loss: 0.39924904165996444 RMSE: 0.63186157\n",
      "24 4 0.3710799217224121\n",
      "24 54 0.26847508549690247\n",
      "24 104 0.26316511631011963\n",
      "24 154 0.42644202709198\n",
      "Validation loss: 0.40687375971012646 RMSE: 0.63786656\n",
      "25 0 0.34676191210746765\n",
      "25 50 0.21285183727741241\n",
      "25 100 0.20969784259796143\n",
      "25 150 0.36507660150527954\n",
      "25 200 0.22311437129974365\n",
      "Validation loss: 0.3831315247548951 RMSE: 0.6189762\n",
      "26 46 0.2943594455718994\n",
      "26 96 0.3342992663383484\n",
      "26 146 0.2231886088848114\n",
      "26 196 0.3742332458496094\n",
      "Validation loss: 0.37470123627119595 RMSE: 0.61212844\n",
      "27 42 0.30901363492012024\n",
      "27 92 0.3433811068534851\n",
      "27 142 0.32208243012428284\n",
      "27 192 0.3145157992839813\n",
      "Validation loss: 0.40585806717475253 RMSE: 0.6370699\n",
      "28 38 0.2702639698982239\n",
      "28 88 0.22576545178890228\n",
      "28 138 0.5075527429580688\n",
      "28 188 0.5354936718940735\n",
      "Validation loss: 0.4195842200683223 RMSE: 0.6477532\n",
      "29 34 0.162496879696846\n",
      "29 84 0.4110627770423889\n",
      "29 134 0.3911787271499634\n",
      "29 184 0.21411490440368652\n",
      "Validation loss: 0.4349246107869678 RMSE: 0.65948814\n",
      "Loaded trained model with success.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/morgan-bert/model_molclr.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2982836689800024 Test RMSE: 0.5461535\n",
      "Running on: cuda:0\n",
      "Loaded pre-trained model with success.\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/morgan-bert/model_molclr.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(checkpoints_folder, 'model.pth'), map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 29.8839111328125\n",
      "0 50 0.3930737376213074\n",
      "0 100 0.33870452642440796\n",
      "0 150 0.26313716173171997\n",
      "0 200 0.6823016405105591\n",
      "Validation loss: 0.5569676665796174 RMSE: 0.7463027\n",
      "1 46 0.4949347972869873\n",
      "1 96 0.49567270278930664\n",
      "1 146 0.19241175055503845\n",
      "1 196 0.34506648778915405\n",
      "Validation loss: 0.375986239562432 RMSE: 0.6131772\n",
      "2 42 0.3618677258491516\n",
      "2 92 0.28397196531295776\n",
      "2 142 0.4659726321697235\n",
      "2 192 0.3034578263759613\n",
      "Validation loss: 0.38614245545532966 RMSE: 0.62140363\n",
      "3 38 0.2710503935813904\n",
      "3 88 0.36545854806900024\n",
      "3 138 0.27476686239242554\n",
      "3 188 0.15806585550308228\n",
      "Validation loss: 0.3816254035466247 RMSE: 0.61775833\n",
      "4 34 0.2906317114830017\n",
      "4 84 0.3255046606063843\n",
      "4 134 0.3829548954963684\n",
      "4 184 0.4112493097782135\n",
      "Validation loss: 0.37729524448513985 RMSE: 0.6142436\n",
      "5 30 0.7859328389167786\n",
      "5 80 0.4958605170249939\n",
      "5 130 0.24172116816043854\n",
      "5 180 0.39381176233291626\n",
      "Validation loss: 0.3875232446524832 RMSE: 0.62251365\n",
      "6 26 0.3264811933040619\n",
      "6 76 0.1747608482837677\n",
      "6 126 0.37281090021133423\n",
      "6 176 0.24449726939201355\n",
      "Validation loss: 0.43203453885184395 RMSE: 0.6572933\n",
      "7 22 0.3273965120315552\n",
      "7 72 0.7082341313362122\n",
      "7 122 0.28382956981658936\n",
      "7 172 0.5121384859085083\n",
      "Validation loss: 0.4226011050244172 RMSE: 0.65007776\n",
      "8 18 0.1413423717021942\n",
      "8 68 1.0442036390304565\n",
      "8 118 0.9901846051216125\n",
      "8 168 0.34198689460754395\n",
      "Validation loss: 0.3548338934779167 RMSE: 0.5956794\n",
      "9 14 0.3323853611946106\n",
      "9 64 0.6468884348869324\n",
      "9 114 0.27988946437835693\n",
      "9 164 0.17053164541721344\n",
      "Validation loss: 0.4416965962284141 RMSE: 0.6646026\n",
      "10 10 0.2720864713191986\n",
      "10 60 0.6107414960861206\n",
      "10 110 0.3672388792037964\n",
      "10 160 0.4286379814147949\n",
      "Validation loss: 0.36778585612773895 RMSE: 0.6064535\n",
      "11 6 0.28372442722320557\n",
      "11 56 0.2637239396572113\n",
      "11 106 0.305736243724823\n",
      "11 156 0.254716157913208\n",
      "Validation loss: 0.37230662297871375 RMSE: 0.61016935\n",
      "12 2 0.2154112458229065\n",
      "12 52 0.27036812901496887\n",
      "12 102 0.30768468976020813\n",
      "12 152 0.23710522055625916\n",
      "12 202 0.36715245246887207\n",
      "Validation loss: 0.518846737841765 RMSE: 0.72031015\n",
      "13 48 0.23642417788505554\n",
      "13 98 0.32315754890441895\n",
      "13 148 0.24219068884849548\n",
      "13 198 0.21145927906036377\n",
      "Validation loss: 0.36104411838783157 RMSE: 0.6008695\n",
      "14 44 0.2503736615180969\n",
      "14 94 0.5708074569702148\n",
      "14 144 0.46778368949890137\n",
      "14 194 0.27725207805633545\n",
      "Validation loss: 0.3435508633653323 RMSE: 0.5861321\n",
      "15 40 0.3363173007965088\n",
      "15 90 0.35986223816871643\n",
      "15 140 0.17575111985206604\n",
      "15 190 0.35783910751342773\n",
      "Validation loss: 0.3956575182576974 RMSE: 0.6290131\n",
      "16 36 0.3982301354408264\n",
      "16 86 0.21692346036434174\n",
      "16 136 0.2704867124557495\n",
      "16 186 0.28195691108703613\n",
      "Validation loss: 0.3887803095082442 RMSE: 0.62352246\n",
      "17 32 0.29100894927978516\n",
      "17 82 0.5221666693687439\n",
      "17 132 0.4616272449493408\n",
      "17 182 0.14038458466529846\n",
      "Validation loss: 0.3554348511000474 RMSE: 0.59618354\n",
      "18 28 0.2615380883216858\n",
      "18 78 0.47954946756362915\n",
      "18 128 0.20515453815460205\n",
      "18 178 0.19714201986789703\n",
      "Validation loss: 0.3433476893438233 RMSE: 0.5859588\n",
      "19 24 0.2483966052532196\n",
      "19 74 0.24864190816879272\n",
      "19 124 0.2585771679878235\n",
      "19 174 0.3018430173397064\n",
      "Validation loss: 0.34984347679548794 RMSE: 0.5914757\n",
      "20 20 0.2920122742652893\n",
      "20 70 0.27155113220214844\n",
      "20 120 0.13778823614120483\n",
      "20 170 0.3283432424068451\n",
      "Validation loss: 0.3420651757882701 RMSE: 0.58486336\n",
      "21 16 0.29483240842819214\n",
      "21 66 0.2895582914352417\n",
      "21 116 0.5269980430603027\n",
      "21 166 0.41883569955825806\n",
      "Validation loss: 0.34623393995894325 RMSE: 0.5884165\n",
      "22 12 0.3427949845790863\n",
      "22 62 0.3588276505470276\n",
      "22 112 0.38292840123176575\n",
      "22 162 0.17714551091194153\n",
      "Validation loss: 0.38298142535818946 RMSE: 0.61885494\n",
      "23 8 0.309565931558609\n",
      "23 58 0.20313617587089539\n",
      "23 108 0.25763171911239624\n",
      "23 158 0.17775161564350128\n",
      "Validation loss: 0.36140765125552815 RMSE: 0.6011719\n",
      "24 4 0.15786996483802795\n",
      "24 54 0.28412890434265137\n",
      "24 104 0.22685156762599945\n",
      "24 154 0.2759018838405609\n",
      "Validation loss: 0.3520572988523377 RMSE: 0.59334415\n",
      "25 0 0.16673541069030762\n",
      "25 50 0.3780514895915985\n",
      "25 100 0.24221131205558777\n",
      "25 150 0.3043568730354309\n",
      "25 200 0.22539022564888\n",
      "Validation loss: 0.3595828368431992 RMSE: 0.5996523\n",
      "26 46 0.15336798131465912\n",
      "26 96 0.2540254294872284\n",
      "26 146 0.32768744230270386\n",
      "26 196 0.3684178292751312\n",
      "Validation loss: 0.3740359346071879 RMSE: 0.6115848\n",
      "27 42 0.18080021440982819\n",
      "27 92 0.3803373873233795\n",
      "27 142 0.24971091747283936\n",
      "27 192 0.29371100664138794\n",
      "Validation loss: 0.3678665527453025 RMSE: 0.60652006\n",
      "28 38 0.21731474995613098\n",
      "28 88 0.31462034583091736\n",
      "28 138 0.15969917178153992\n",
      "28 188 0.4894070327281952\n",
      "Validation loss: 0.3427692163321707 RMSE: 0.585465\n",
      "29 34 0.16274958848953247\n",
      "29 84 0.19493401050567627\n",
      "29 134 0.23384559154510498\n",
      "29 184 0.21564677357673645\n",
      "Validation loss: 0.3493305163251029 RMSE: 0.5910419\n",
      "Loaded trained model with success.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/morgan-bert/model_molclr.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3187332794070244 Test RMSE: 0.5645647\n",
      "Running on: cuda:0\n",
      "Loaded pre-trained model with success.\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/morgan-bert/model_molclr.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(checkpoints_folder, 'model.pth'), map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 27.308486938476562\n",
      "0 50 0.34748196601867676\n",
      "0 100 0.25863105058670044\n",
      "0 150 0.182604119181633\n",
      "0 200 0.3906329870223999\n",
      "Validation loss: 0.4339016204078992 RMSE: 0.6587121\n",
      "1 46 0.4808252453804016\n",
      "1 96 0.30967479944229126\n",
      "1 146 0.37554502487182617\n",
      "1 196 0.36395299434661865\n",
      "Validation loss: 0.3971746137572659 RMSE: 0.6302179\n",
      "2 42 0.3374921679496765\n",
      "2 92 0.2662486433982849\n",
      "2 142 0.2948164939880371\n",
      "2 192 0.5649024248123169\n",
      "Validation loss: 0.3867053155683809 RMSE: 0.62185633\n",
      "3 38 0.3324461579322815\n",
      "3 88 0.48419272899627686\n",
      "3 138 0.19962260127067566\n",
      "3 188 0.5112130045890808\n",
      "Validation loss: 0.3725813925266266 RMSE: 0.6103945\n",
      "4 34 0.24251016974449158\n",
      "4 84 0.17593026161193848\n",
      "4 134 0.38421720266342163\n",
      "4 184 0.33845606446266174\n",
      "Validation loss: 0.3708218216068215 RMSE: 0.60895145\n",
      "5 30 0.5412824153900146\n",
      "5 80 0.29728108644485474\n",
      "5 130 0.14410164952278137\n",
      "5 180 0.2526898980140686\n",
      "Validation loss: 0.45802803834279376 RMSE: 0.6767777\n",
      "6 26 0.20744316279888153\n",
      "6 76 0.5863826870918274\n",
      "6 126 0.2573847770690918\n",
      "6 176 0.3795464038848877\n",
      "Validation loss: 0.37872277448574704 RMSE: 0.61540455\n",
      "7 22 0.3645319938659668\n",
      "7 72 0.5409286022186279\n",
      "7 122 0.3651866614818573\n",
      "7 172 0.25337961316108704\n",
      "Validation loss: 0.35807782949672806 RMSE: 0.59839606\n",
      "8 18 0.31650641560554504\n",
      "8 68 0.2229393571615219\n",
      "8 118 0.2503108084201813\n",
      "8 168 0.3266049325466156\n",
      "Validation loss: 0.389504615838329 RMSE: 0.624103\n",
      "9 14 0.4480055868625641\n",
      "9 64 0.4632875621318817\n",
      "9 114 0.22784537076950073\n",
      "9 164 0.3952723741531372\n",
      "Validation loss: 0.3839137777686119 RMSE: 0.61960775\n",
      "10 10 0.3706795275211334\n",
      "10 60 0.39442649483680725\n",
      "10 110 0.4405907094478607\n",
      "10 160 0.24333974719047546\n",
      "Validation loss: 0.34923384090264636 RMSE: 0.5909601\n",
      "11 6 0.4851183295249939\n",
      "11 56 0.7531134486198425\n",
      "11 106 0.2963221073150635\n",
      "11 156 0.21313315629959106\n",
      "Validation loss: 0.3816820416185591 RMSE: 0.6178042\n",
      "12 2 0.3302411437034607\n",
      "12 52 0.32587766647338867\n",
      "12 102 0.3553776741027832\n",
      "12 152 0.2683395743370056\n",
      "12 202 0.48700404167175293\n",
      "Validation loss: 0.3703971550696426 RMSE: 0.60860264\n",
      "13 48 0.5217061638832092\n",
      "13 98 0.4772375226020813\n",
      "13 148 0.11749087274074554\n",
      "13 198 0.19226059317588806\n",
      "Validation loss: 0.3638315126299858 RMSE: 0.60318446\n",
      "14 44 0.27164748311042786\n",
      "14 94 0.34714075922966003\n",
      "14 144 0.17755982279777527\n",
      "14 194 0.43846452236175537\n",
      "Validation loss: 0.3750747736129496 RMSE: 0.6124335\n",
      "15 40 0.5663363933563232\n",
      "15 90 0.2878761887550354\n",
      "15 140 0.3702477216720581\n",
      "15 190 0.3314664959907532\n",
      "Validation loss: 0.3535230904817581 RMSE: 0.5945781\n",
      "16 36 0.4480627179145813\n",
      "16 86 0.2694607973098755\n",
      "16 136 0.2664520740509033\n",
      "16 186 0.22944210469722748\n",
      "Validation loss: 0.3438057601451874 RMSE: 0.58634955\n",
      "17 32 0.2598259150981903\n",
      "17 82 0.5866754651069641\n",
      "17 132 0.35176312923431396\n",
      "17 182 0.3497723340988159\n",
      "Validation loss: 0.33878667280077934 RMSE: 0.58205384\n",
      "18 28 0.2294626533985138\n",
      "18 78 0.32166093587875366\n",
      "18 128 0.20408305525779724\n",
      "18 178 0.39619678258895874\n",
      "Validation loss: 0.39625080219573444 RMSE: 0.62948453\n",
      "19 24 0.6509591341018677\n",
      "19 74 0.1592288315296173\n",
      "19 124 0.30773991346359253\n",
      "19 174 0.26582208275794983\n",
      "Validation loss: 0.35846976687510806 RMSE: 0.5987234\n",
      "20 20 0.294807493686676\n",
      "20 70 0.22784960269927979\n",
      "20 120 0.36526405811309814\n",
      "20 170 0.30371567606925964\n",
      "Validation loss: 0.3441103340850936 RMSE: 0.5866092\n",
      "21 16 0.284231036901474\n",
      "21 66 0.2154245376586914\n",
      "21 116 0.2887471914291382\n",
      "21 166 0.33154886960983276\n",
      "Validation loss: 0.3407461241715484 RMSE: 0.58373463\n",
      "22 12 0.3413124978542328\n",
      "22 62 0.26979416608810425\n",
      "22 112 0.30713367462158203\n",
      "22 162 0.2082517296075821\n",
      "Validation loss: 0.3982927016913891 RMSE: 0.63110435\n",
      "23 8 0.2423500269651413\n",
      "23 58 0.18801814317703247\n",
      "23 108 0.22477680444717407\n",
      "23 158 0.17734599113464355\n",
      "Validation loss: 0.36715319711301064 RMSE: 0.6059317\n",
      "24 4 0.21334199607372284\n",
      "24 54 0.29187825322151184\n",
      "24 104 0.3527011573314667\n",
      "24 154 0.22996951639652252\n",
      "Validation loss: 0.3371999077498913 RMSE: 0.58068913\n",
      "25 0 0.7307773232460022\n",
      "25 50 0.3561321198940277\n",
      "25 100 0.40312910079956055\n",
      "25 150 0.2888347804546356\n",
      "25 200 0.5502070784568787\n",
      "Validation loss: 0.3469991009268496 RMSE: 0.58906627\n",
      "26 46 0.2613554000854492\n",
      "26 96 0.2502600848674774\n",
      "26 146 0.4281339645385742\n",
      "26 196 0.43422508239746094\n",
      "Validation loss: 0.33660319447517395 RMSE: 0.58017516\n",
      "27 42 0.3366461396217346\n",
      "27 92 0.17334415018558502\n",
      "27 142 0.3036147654056549\n",
      "27 192 0.618012547492981\n",
      "Validation loss: 0.3341165876222981 RMSE: 0.5780282\n",
      "28 38 0.19293594360351562\n",
      "28 88 0.5480639934539795\n",
      "28 138 0.2493821382522583\n",
      "28 188 0.41952192783355713\n",
      "Validation loss: 0.3425929674671756 RMSE: 0.5853144\n",
      "29 34 0.32221364974975586\n",
      "29 84 0.17593331634998322\n",
      "29 134 0.3933314383029938\n",
      "29 184 0.4320877492427826\n",
      "Validation loss: 0.36643949564960265 RMSE: 0.60534245\n",
      "Loaded trained model with success.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/morgan-bert/model_molclr.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2868800155818462 Test RMSE: 0.5356118\n",
      "Running on: cuda:0\n",
      "Loaded pre-trained model with success.\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/morgan-bert/model_molclr.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(checkpoints_folder, 'model.pth'), map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 30.343385696411133\n",
      "0 50 0.835064172744751\n",
      "0 100 0.27465298771858215\n",
      "0 150 0.3000343441963196\n",
      "0 200 0.26801246404647827\n",
      "Validation loss: 0.33456486091017723 RMSE: 0.5784158\n",
      "1 46 0.4803263545036316\n",
      "1 96 0.25407737493515015\n",
      "1 146 0.6356313228607178\n",
      "1 196 0.28638210892677307\n",
      "Validation loss: 0.4621350169181824 RMSE: 0.6798051\n",
      "2 42 0.2908737361431122\n",
      "2 92 0.3706163167953491\n",
      "2 142 0.27434444427490234\n",
      "2 192 0.8193899989128113\n",
      "Validation loss: 0.34055351548724705 RMSE: 0.58356965\n",
      "3 38 0.5023944973945618\n",
      "3 88 0.38451385498046875\n",
      "3 138 0.1542980968952179\n",
      "3 188 0.272146999835968\n",
      "Validation loss: 0.3120538762046231 RMSE: 0.55861783\n",
      "4 34 0.3305756449699402\n",
      "4 84 0.3274973928928375\n",
      "4 134 0.45750871300697327\n",
      "4 184 0.26879018545150757\n",
      "Validation loss: 0.32262328184313244 RMSE: 0.5679994\n",
      "5 30 0.4650845527648926\n",
      "5 80 0.5517879128456116\n",
      "5 130 0.4760979115962982\n",
      "5 180 0.2967544198036194\n",
      "Validation loss: 0.3715222345458137 RMSE: 0.6095262\n",
      "6 26 0.40944525599479675\n",
      "6 76 0.28531864285469055\n",
      "6 126 0.35671061277389526\n",
      "6 176 0.22958503663539886\n",
      "Validation loss: 0.33542335737082696 RMSE: 0.5791575\n",
      "7 22 0.36776161193847656\n",
      "7 72 0.2564854025840759\n",
      "7 122 0.5265862345695496\n",
      "7 172 0.5022691488265991\n",
      "Validation loss: 0.34287000074982643 RMSE: 0.585551\n",
      "8 18 0.26385200023651123\n",
      "8 68 0.33587372303009033\n",
      "8 118 0.4366115629673004\n",
      "8 168 0.2510541081428528\n",
      "Validation loss: 0.2924348521563742 RMSE: 0.54077244\n",
      "9 14 0.27774348855018616\n",
      "9 64 0.3376448154449463\n",
      "9 114 0.23248447477817535\n",
      "9 164 0.2905402183532715\n",
      "Validation loss: 0.407888042430083 RMSE: 0.63866115\n",
      "10 10 0.2943476736545563\n",
      "10 60 0.2914592921733856\n",
      "10 110 0.24711711704730988\n",
      "10 160 0.6149092316627502\n",
      "Validation loss: 0.3033950933151775 RMSE: 0.55081314\n",
      "11 6 0.686387836933136\n",
      "11 56 0.31031060218811035\n",
      "11 106 0.396517276763916\n",
      "11 156 0.174727201461792\n",
      "Validation loss: 0.3801303398278024 RMSE: 0.6165471\n",
      "12 2 0.2814345359802246\n",
      "12 52 0.6341854929924011\n",
      "12 102 0.39977481961250305\n",
      "12 152 0.48717737197875977\n",
      "12 202 0.18210366368293762\n",
      "Validation loss: 0.31435111661752063 RMSE: 0.56067026\n",
      "13 48 0.16578811407089233\n",
      "13 98 0.43838340044021606\n",
      "13 148 0.3317379057407379\n",
      "13 198 0.22403836250305176\n",
      "Validation loss: 0.3201819492710961 RMSE: 0.56584626\n",
      "14 44 0.25954297184944153\n",
      "14 94 0.20166946947574615\n",
      "14 144 0.2586226463317871\n",
      "14 194 0.30326685309410095\n",
      "Validation loss: 0.283295354909367 RMSE: 0.53225493\n",
      "15 40 0.18505248427391052\n",
      "15 90 0.17864938080310822\n",
      "15 140 0.6981233358383179\n",
      "15 190 0.26917827129364014\n",
      "Validation loss: 0.28230901517801815 RMSE: 0.5313276\n",
      "16 36 0.3626333475112915\n",
      "16 86 0.3201466202735901\n",
      "16 136 0.21543681621551514\n",
      "16 186 0.33262544870376587\n",
      "Validation loss: 0.28376100584864616 RMSE: 0.53269225\n",
      "17 32 0.29592442512512207\n",
      "17 82 0.4032391607761383\n",
      "17 132 0.19516652822494507\n",
      "17 182 0.38721516728401184\n",
      "Validation loss: 0.2810656883650356 RMSE: 0.5301563\n",
      "18 28 0.2793636620044708\n",
      "18 78 0.3531425893306732\n",
      "18 128 0.28798890113830566\n",
      "18 178 0.23776811361312866\n",
      "Validation loss: 0.2856662596265475 RMSE: 0.53447753\n",
      "19 24 0.2583242952823639\n",
      "19 74 0.44435232877731323\n",
      "19 124 0.23575013875961304\n",
      "19 174 0.46279382705688477\n",
      "Validation loss: 0.28961212891671395 RMSE: 0.5381563\n",
      "20 20 0.22571319341659546\n",
      "20 70 0.2576444149017334\n",
      "20 120 0.2948955297470093\n",
      "20 170 0.30338287353515625\n",
      "Validation loss: 0.2916755970153544 RMSE: 0.54007\n",
      "21 16 0.32204240560531616\n",
      "21 66 0.3667890429496765\n",
      "21 116 0.5485294461250305\n",
      "21 166 0.1939520537853241\n",
      "Validation loss: 0.28627144752277267 RMSE: 0.5350434\n",
      "22 12 0.35927167534828186\n",
      "22 62 0.5761821866035461\n",
      "22 112 0.6138935089111328\n",
      "22 162 0.328582227230072\n",
      "Validation loss: 0.2973131616082456 RMSE: 0.5452643\n",
      "23 8 0.3182409405708313\n",
      "23 58 0.5402622222900391\n",
      "23 108 0.4791605770587921\n",
      "23 158 0.22222425043582916\n",
      "Validation loss: 0.296573705971241 RMSE: 0.5445858\n",
      "24 4 0.5164110660552979\n",
      "24 54 0.655360758304596\n",
      "24 104 0.23514796793460846\n",
      "24 154 0.11128570139408112\n",
      "Validation loss: 0.2840096491078536 RMSE: 0.53292555\n",
      "25 0 0.240568608045578\n",
      "25 50 0.20341706275939941\n",
      "25 100 0.2342296540737152\n",
      "25 150 0.21093496680259705\n",
      "25 200 0.17922955751419067\n",
      "Validation loss: 0.3029407389048073 RMSE: 0.5504005\n",
      "26 46 0.31800082325935364\n",
      "26 96 0.21245688199996948\n",
      "26 146 0.13610689342021942\n",
      "26 196 0.24007287621498108\n",
      "Validation loss: 0.3120831801659531 RMSE: 0.55864406\n",
      "27 42 0.2537415623664856\n",
      "27 92 0.30913975834846497\n",
      "27 142 0.11969894170761108\n",
      "27 192 0.3954015374183655\n",
      "Validation loss: 0.2681783644689454 RMSE: 0.5178594\n",
      "28 38 0.2707329988479614\n",
      "28 88 0.2941487431526184\n",
      "28 138 0.13755884766578674\n",
      "28 188 0.30225902795791626\n",
      "Validation loss: 0.3225245608223809 RMSE: 0.56791246\n",
      "29 34 0.3281964957714081\n",
      "29 84 0.20023857057094574\n",
      "29 134 0.2686690390110016\n",
      "29 184 0.2945626676082611\n",
      "Validation loss: 0.30573185595373314 RMSE: 0.55293024\n",
      "Loaded trained model with success.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/morgan-bert/model_molclr.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.31187201142311094 Test RMSE: 0.55845505\n",
      "Running on: cuda:0\n",
      "Loaded pre-trained model with success.\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/morgan-bert/model_molclr.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(os.path.join(checkpoints_folder, 'model.pth'), map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 36.43439483642578\n",
      "0 50 0.7695717215538025\n",
      "0 100 0.3472138047218323\n",
      "0 150 0.5931179523468018\n",
      "0 200 0.5816248655319214\n",
      "Validation loss: 0.48353451242049533 RMSE: 0.69536644\n",
      "1 46 0.36909908056259155\n",
      "1 96 0.41472214460372925\n",
      "1 146 0.2857234477996826\n",
      "1 196 0.36150968074798584\n",
      "Validation loss: 0.42004451983504826 RMSE: 0.6481084\n",
      "2 42 0.5663822889328003\n",
      "2 92 0.3346196711063385\n",
      "2 142 0.30270034074783325\n",
      "2 192 0.3720460832118988\n",
      "Validation loss: 0.3521901418765386 RMSE: 0.5934561\n",
      "3 38 0.22314319014549255\n",
      "3 88 0.37405329942703247\n",
      "3 138 0.2159474790096283\n",
      "3 188 0.18982872366905212\n",
      "Validation loss: 0.437839408715566 RMSE: 0.66169435\n",
      "4 34 0.3669510781764984\n",
      "4 84 0.39448145031929016\n",
      "4 134 0.47786828875541687\n",
      "4 184 0.1756317913532257\n",
      "Validation loss: 0.3658553966217571 RMSE: 0.6048598\n",
      "5 30 0.3101109266281128\n",
      "5 80 0.2025982141494751\n",
      "5 130 0.19214403629302979\n",
      "5 180 0.21169966459274292\n",
      "Validation loss: 0.3414742731385761 RMSE: 0.584358\n",
      "6 26 0.3687085509300232\n",
      "6 76 0.28480297327041626\n",
      "6 126 0.20084011554718018\n",
      "6 176 0.31951045989990234\n",
      "Validation loss: 0.5793685929642783 RMSE: 0.76116264\n",
      "7 22 0.4219980239868164\n",
      "7 72 0.3778795599937439\n",
      "7 122 0.2536543309688568\n",
      "7 172 0.3220830261707306\n",
      "Validation loss: 0.36370876369376975 RMSE: 0.6030827\n",
      "8 18 0.8157114386558533\n",
      "8 68 0.2621309757232666\n",
      "8 118 0.1253507137298584\n",
      "8 168 0.4500279128551483\n",
      "Validation loss: 0.3834504712786939 RMSE: 0.6192338\n",
      "9 14 0.3172382116317749\n",
      "9 64 0.1321556121110916\n",
      "9 114 0.5182064175605774\n",
      "9 164 0.34470897912979126\n",
      "Validation loss: 0.3580379868961043 RMSE: 0.59836274\n",
      "10 10 0.24156177043914795\n",
      "10 60 0.4033348560333252\n",
      "10 110 0.6612961292266846\n",
      "10 160 0.14273701608181\n",
      "Validation loss: 0.40057164803147316 RMSE: 0.6329073\n",
      "11 6 0.3656512498855591\n",
      "11 56 0.31280970573425293\n",
      "11 106 0.3152778148651123\n",
      "11 156 0.20908723771572113\n",
      "Validation loss: 0.4519824865791533 RMSE: 0.67229646\n",
      "12 2 0.35241496562957764\n",
      "12 52 0.22544726729393005\n",
      "12 102 0.36456185579299927\n",
      "12 152 0.19564059376716614\n",
      "12 202 0.24632468819618225\n",
      "Validation loss: 0.35549606962336433 RMSE: 0.5962349\n",
      "13 48 0.25505539774894714\n",
      "13 98 0.21649330854415894\n",
      "13 148 0.30589988827705383\n",
      "13 198 0.5280593633651733\n",
      "Validation loss: 0.38672999375396305 RMSE: 0.6218762\n",
      "14 44 0.2150118052959442\n",
      "14 94 0.11367924511432648\n",
      "14 144 0.2918250560760498\n",
      "14 194 0.3320310711860657\n",
      "Validation loss: 0.4462936297059059 RMSE: 0.66805214\n",
      "15 40 0.2303922027349472\n",
      "15 90 0.4403683841228485\n",
      "15 140 0.4166073203086853\n",
      "15 190 0.3186624050140381\n",
      "Validation loss: 0.37140603446298176 RMSE: 0.6094309\n",
      "16 36 0.22473075985908508\n",
      "16 86 0.7219371795654297\n",
      "16 136 0.20822343230247498\n",
      "16 186 0.34908002614974976\n",
      "Validation loss: 0.3742181787060367 RMSE: 0.61173373\n",
      "17 32 0.26144668459892273\n",
      "17 82 0.45373213291168213\n",
      "17 132 0.4368439018726349\n",
      "17 182 0.20187406241893768\n",
      "Validation loss: 0.35345496775375473 RMSE: 0.5945208\n",
      "18 28 0.30206164717674255\n",
      "18 78 0.6350439190864563\n",
      "18 128 0.22909174859523773\n",
      "18 178 0.32763907313346863\n",
      "Validation loss: 0.3950516672597991 RMSE: 0.62853134\n",
      "19 24 0.24262648820877075\n",
      "19 74 0.6641079783439636\n",
      "19 124 0.25112009048461914\n",
      "19 174 0.3078649044036865\n",
      "Validation loss: 0.3545777106450664 RMSE: 0.5954642\n",
      "20 20 0.21907222270965576\n",
      "20 70 0.4304133653640747\n",
      "20 120 0.47643351554870605\n",
      "20 170 0.20867863297462463\n",
      "Validation loss: 0.41702518115441006 RMSE: 0.6457749\n",
      "21 16 0.19634601473808289\n",
      "21 66 0.2994244694709778\n",
      "21 116 0.20516589283943176\n",
      "21 166 0.37912869453430176\n",
      "Validation loss: 0.35729876077837414 RMSE: 0.5977447\n",
      "22 12 0.31320711970329285\n",
      "22 62 0.2651020586490631\n",
      "22 112 0.16231292486190796\n",
      "22 162 0.49750781059265137\n",
      "Validation loss: 0.35105285172661144 RMSE: 0.59249717\n",
      "23 8 0.1920567750930786\n",
      "23 58 0.3489726781845093\n",
      "23 108 0.32560357451438904\n",
      "23 158 0.18181660771369934\n",
      "Validation loss: 0.35937722937928307 RMSE: 0.5994808\n",
      "24 4 0.25169169902801514\n",
      "24 54 0.41147303581237793\n",
      "24 104 0.25842148065567017\n",
      "24 154 0.1753000020980835\n",
      "Validation loss: 0.4195571218927701 RMSE: 0.6477323\n",
      "25 0 0.31339719891548157\n",
      "25 50 0.6676421165466309\n",
      "25 100 0.2655235528945923\n",
      "25 150 0.39567676186561584\n",
      "25 200 0.2327938675880432\n",
      "Validation loss: 0.4526949818763468 RMSE: 0.6728261\n",
      "26 46 0.16708269715309143\n",
      "26 96 0.6150146722793579\n",
      "26 146 0.22884634137153625\n",
      "26 196 0.17199033498764038\n",
      "Validation loss: 0.3939790858162774 RMSE: 0.6276775\n",
      "27 42 0.34605246782302856\n",
      "27 92 0.7125078439712524\n",
      "27 142 0.5884798169136047\n",
      "27 192 0.5077763795852661\n",
      "Validation loss: 0.35566661444803077 RMSE: 0.5963779\n",
      "28 38 0.42348533868789673\n",
      "28 88 0.3879748284816742\n",
      "28 138 0.34404683113098145\n",
      "28 188 0.3165125250816345\n",
      "Validation loss: 0.35324134139551056 RMSE: 0.5943411\n",
      "29 34 0.528038740158081\n",
      "29 84 0.23842915892601013\n",
      "29 134 0.19265905022621155\n",
      "29 184 0.1599634289741516\n",
      "Validation loss: 0.35519752237531876 RMSE: 0.5959845\n",
      "Loaded trained model with success.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/morgan-bert/model_molclr.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=self.device)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-10M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.34048304210106534 Test RMSE: 0.58350927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 06:18, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.856300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.329400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.302200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.280700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.265700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.245200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-10M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 06:22, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.303900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.283100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.272700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.252000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-10M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 06:10, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.334300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.310600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.288900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.256100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-10M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 06:06, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.709500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.335800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.305400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.284400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.272100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.253300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-10M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 06:15, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.696200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.331700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.303400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.259700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.252600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 06:21, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.330300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.314700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.295700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.289800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 06:17, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.501600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.353400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.328000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.314600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.291100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.289400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 06:09, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.351600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.333600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.306900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.295200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.290500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 06:10, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.489100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.329700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.314800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.295400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.290600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 06:07, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.485500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.349500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.327100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.315500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.301700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.295800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.289300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_1_s_0_atomFirst_f_300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 20:26, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.058400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.031300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_1_s_0_atomFirst_f_300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 20:24, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.510800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.174300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.090100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.062200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.032700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_1_s_0_atomFirst_f_300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 20:22, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.503200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.048100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.039600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_1_s_0_atomFirst_f_300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 20:23, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.511400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.167100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.090800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.032700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_1_s_0_atomFirst_f_300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3600' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3600/3600 20:23, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.506900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.178300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.033800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_1_s_0_atomFirst_f_20 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1674' max='3600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1674/3600 09:27 < 10:53, 2.95 it/s, Epoch 13.94/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.167100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.087900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'RF+MACCS'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'RF+PubChemFP'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'RF+ECFP2'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'RF+ECFP4'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'RF+Daylight'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'RF+RDKitFP'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'D-MPNN'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'ChemBERTa-10M-MLM'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'ChemBERTa-77M-MLM'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_perf = utils_benchmark.run_benchmark(task_name, model_perf, data_dir='./benchmarks/')\n",
    "len(model_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27ba9eca-7b9f-477e-8dc2-b8fb2d1c72d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of compounds: 9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RF+Mol2vec: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:32<00:00, 18.54s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_perf = utils_benchmark.run_benchmark(task_name, model_perf)\n",
    "len(model_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8db064a-71fe-452c-937d-3a7f19e94a17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- eval_mse --------------------\n",
      "KNN 0.364 $\\pm$ 0.023\n",
      "RF+MACCS 0.326 $\\pm$ 0.008\n",
      "RF+PubChemFP 0.314 $\\pm$ 0.012\n",
      "RF+ECFP2 0.305 $\\pm$ 0.009\n",
      "RF+ECFP4 0.304 $\\pm$ 0.010\n",
      "RF+Daylight 0.314 $\\pm$ 0.009\n",
      "RF+RDKitFP 0.308 $\\pm$ 0.009\n",
      "RF+Mol2vec 0.319 $\\pm$ 0.004\n",
      "D-MPNN 0.319 $\\pm$ 0.020\n",
      "MolCLR 0.311 $\\pm$ 0.020\n",
      "ChemBERTa-10M-MLM 0.327 $\\pm$ 0.025\n",
      "ChemBERTa-77M-MLM 0.322 $\\pm$ 0.007\n",
      "MolFormer 0.406 $\\pm$ 0.044\n",
      "MorganBERT_base_full_r_0_s_0 0.332 $\\pm$ 0.012\n",
      "MorganBERT_base_full_r_1_s_0_atomFirst_f_300 0.340 $\\pm$ 0.022\n",
      "MorganBERT_base_full_r_1_s_0_radiusFirst_f_300 0.330 $\\pm$ 0.008\n",
      "MorganBERT_base_full_r_2_s_0_atomFirst_f_2300 0.304 $\\pm$ 0.014\n",
      "MorganBERT_base_full_r_2_s_0_radiusFirst_f_2300 0.330 $\\pm$ 0.010\n",
      "\n",
      "\n",
      "-------------------- eval_mae --------------------\n",
      "KNN 0.454 $\\pm$ 0.015\n",
      "RF+MACCS 0.436 $\\pm$ 0.004\n",
      "RF+PubChemFP 0.425 $\\pm$ 0.009\n",
      "RF+ECFP2 0.415 $\\pm$ 0.007\n",
      "RF+ECFP4 0.414 $\\pm$ 0.008\n",
      "RF+Daylight 0.432 $\\pm$ 0.007\n",
      "RF+RDKitFP 0.426 $\\pm$ 0.007\n",
      "RF+Mol2vec 0.435 $\\pm$ 0.006\n",
      "D-MPNN 0.419 $\\pm$ 0.014\n",
      "MolCLR 0.420 $\\pm$ 0.020\n",
      "ChemBERTa-10M-MLM 0.441 $\\pm$ 0.026\n",
      "ChemBERTa-77M-MLM 0.439 $\\pm$ 0.008\n",
      "MolFormer 0.511 $\\pm$ 0.035\n",
      "MorganBERT_base_full_r_0_s_0 0.430 $\\pm$ 0.011\n",
      "MorganBERT_base_full_r_1_s_0_atomFirst_f_300 0.437 $\\pm$ 0.019\n",
      "MorganBERT_base_full_r_1_s_0_radiusFirst_f_300 0.429 $\\pm$ 0.005\n",
      "MorganBERT_base_full_r_2_s_0_atomFirst_f_2300 0.404 $\\pm$ 0.010\n",
      "MorganBERT_base_full_r_2_s_0_radiusFirst_f_2300 0.435 $\\pm$ 0.008\n",
      "\n",
      "\n",
      "-------------------- eval_r2 --------------------\n",
      "KNN 0.096 $\\pm$ 0.071\n",
      "RF+MACCS 0.190 $\\pm$ 0.026\n",
      "RF+PubChemFP 0.220 $\\pm$ 0.041\n",
      "RF+ECFP2 0.243 $\\pm$ 0.036\n",
      "RF+ECFP4 0.245 $\\pm$ 0.039\n",
      "RF+Daylight 0.221 $\\pm$ 0.034\n",
      "RF+RDKitFP 0.236 $\\pm$ 0.035\n",
      "RF+Mol2vec 0.207 $\\pm$ 0.021\n",
      "D-MPNN 0.207 $\\pm$ 0.062\n",
      "MolCLR 0.228 $\\pm$ 0.043\n",
      "ChemBERTa-10M-MLM 0.189 $\\pm$ 0.069\n",
      "ChemBERTa-77M-MLM 0.200 $\\pm$ 0.027\n",
      "MolFormer -0.875 $\\pm$ 0.146\n",
      "MorganBERT_base_full_r_0_s_0 0.177 $\\pm$ 0.032\n",
      "MorganBERT_base_full_r_1_s_0_atomFirst_f_300 0.157 $\\pm$ 0.065\n",
      "MorganBERT_base_full_r_1_s_0_radiusFirst_f_300 0.180 $\\pm$ 0.034\n",
      "MorganBERT_base_full_r_2_s_0_atomFirst_f_2300 0.244 $\\pm$ 0.051\n",
      "MorganBERT_base_full_r_2_s_0_radiusFirst_f_2300 0.182 $\\pm$ 0.044\n",
      "\n",
      "\n",
      "-------------------- eval_rmse --------------------\n",
      "KNN 0.603 $\\pm$ 0.019\n",
      "RF+MACCS 0.571 $\\pm$ 0.007\n",
      "RF+PubChemFP 0.560 $\\pm$ 0.011\n",
      "RF+ECFP2 0.552 $\\pm$ 0.008\n",
      "RF+ECFP4 0.551 $\\pm$ 0.009\n",
      "RF+Daylight 0.560 $\\pm$ 0.008\n",
      "RF+RDKitFP 0.555 $\\pm$ 0.008\n",
      "RF+Mol2vec 0.565 $\\pm$ 0.004\n",
      "D-MPNN 0.565 $\\pm$ 0.017\n",
      "MolCLR 0.558 $\\pm$ 0.018\n",
      "ChemBERTa-10M-MLM 0.571 $\\pm$ 0.022\n",
      "ChemBERTa-77M-MLM 0.568 $\\pm$ 0.006\n",
      "MolFormer 0.636 $\\pm$ 0.035\n",
      "MorganBERT_base_full_r_0_s_0 0.576 $\\pm$ 0.010\n",
      "MorganBERT_base_full_r_1_s_0_atomFirst_f_300 0.582 $\\pm$ 0.019\n",
      "MorganBERT_base_full_r_1_s_0_radiusFirst_f_300 0.575 $\\pm$ 0.007\n",
      "MorganBERT_base_full_r_2_s_0_atomFirst_f_2300 0.552 $\\pm$ 0.013\n",
      "MorganBERT_base_full_r_2_s_0_radiusFirst_f_2300 0.574 $\\pm$ 0.009\n",
      "\n",
      "\n",
      "-------------------- eval_pcc --------------------\n",
      "KNN 0.429 $\\pm$ 0.051\n",
      "RF+MACCS 0.454 $\\pm$ 0.025\n",
      "RF+PubChemFP 0.475 $\\pm$ 0.040\n",
      "RF+ECFP2 0.495 $\\pm$ 0.034\n",
      "RF+ECFP4 0.497 $\\pm$ 0.038\n",
      "RF+Daylight 0.485 $\\pm$ 0.033\n",
      "RF+RDKitFP 0.499 $\\pm$ 0.036\n",
      "RF+Mol2vec 0.459 $\\pm$ 0.020\n",
      "D-MPNN 0.521 $\\pm$ 0.045\n",
      "MolCLR 0.484 $\\pm$ 0.044\n",
      "ChemBERTa-10M-MLM 0.510 $\\pm$ 0.035\n",
      "ChemBERTa-77M-MLM 0.506 $\\pm$ 0.027\n",
      "MolFormer 0.529 $\\pm$ 0.032\n",
      "MorganBERT_base_full_r_0_s_0 0.505 $\\pm$ 0.023\n",
      "MorganBERT_base_full_r_1_s_0_atomFirst_f_300 0.522 $\\pm$ 0.034\n",
      "MorganBERT_base_full_r_1_s_0_radiusFirst_f_300 0.542 $\\pm$ 0.030\n",
      "MorganBERT_base_full_r_2_s_0_atomFirst_f_2300 0.546 $\\pm$ 0.039\n",
      "MorganBERT_base_full_r_2_s_0_radiusFirst_f_2300 0.544 $\\pm$ 0.040\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils_benchmark.print_perf_table(model_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a02a3-b9fa-4443-85e7-cae66250b932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
