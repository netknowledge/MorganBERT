{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b760368-8bf4-433e-90f1-80882d99fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e432dba-1d8b-4e91-9ee9-ccde307ce7b2",
   "metadata": {},
   "source": [
    "## NCATS-LD50-mouse-intraperitoneal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93cde620-e68b-4b5e-addd-92f23cb1b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'NCATS-LD50-mouse-intraperitoneal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0420d382-f040-465b-9f34-837ab8eab448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncats_mouse_ip_model_perf = utils_benchmark.load_model_perf(task_name)\n",
    "len(ncats_mouse_ip_model_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe7d1b-b9aa-4d66-96ac-0f0730e8a7c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:71: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "You are using a CUDA device ('NVIDIA A800 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type               | Params\n",
      "-------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K \n",
      "1 | agg             | MeanAggregation    | 0     \n",
      "2 | bn              | BatchNorm1d        | 600   \n",
      "3 | predictor       | RegressionFFN      | 90.6 K\n",
      "4 | X_d_transform   | Identity           | 0     \n",
      "-------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e78d1f050f433aa937353c75ec2565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7103046a15c044148fe0c3fe861d851a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:71: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type               | Params\n",
      "-------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K \n",
      "1 | agg             | MeanAggregation    | 0     \n",
      "2 | bn              | BatchNorm1d        | 600   \n",
      "3 | predictor       | RegressionFFN      | 90.6 K\n",
      "4 | X_d_transform   | Identity           | 0     \n",
      "-------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9954339329c40f094ce239d92dc30de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15679581b45a4ecdac8ffcf4e8876049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:71: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type               | Params\n",
      "-------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K \n",
      "1 | agg             | MeanAggregation    | 0     \n",
      "2 | bn              | BatchNorm1d        | 600   \n",
      "3 | predictor       | RegressionFFN      | 90.6 K\n",
      "4 | X_d_transform   | Identity           | 0     \n",
      "-------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b921097189074a028d341560ab77a534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e634ba5e6d264d3d9304af04361ab83d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:71: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type               | Params\n",
      "-------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K \n",
      "1 | agg             | MeanAggregation    | 0     \n",
      "2 | bn              | BatchNorm1d        | 600   \n",
      "3 | predictor       | RegressionFFN      | 90.6 K\n",
      "4 | X_d_transform   | Identity           | 0     \n",
      "-------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df54f81364a4d0ea1e9e1bbee6531c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1b81c144364b318b93f652df498ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:71: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type               | Params\n",
      "-------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing | 227 K \n",
      "1 | agg             | MeanAggregation    | 0     \n",
      "2 | bn              | BatchNorm1d        | 600   \n",
      "3 | predictor       | RegressionFFN      | 90.6 K\n",
      "4 | X_d_transform   | Identity           | 0     \n",
      "-------------------------------------------------------\n",
      "318 K     Trainable params\n",
      "0         Non-trainable params\n",
      "318 K     Total params\n",
      "1.276     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5829c2294916437ba1c6b199698606b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6452aaa14fa14749ba422c674e4fef30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda:0\n",
      "Loaded pre-trained model with success.\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 11.302289962768555\n",
      "0 50 0.2680557370185852\n",
      "0 100 0.5883915424346924\n",
      "0 150 0.30780041217803955\n",
      "0 200 0.4693502187728882\n",
      "0 250 0.32158225774765015\n",
      "0 300 0.4554862380027771\n",
      "0 350 0.439075231552124\n",
      "0 400 0.4864735007286072\n",
      "0 450 0.338773638010025\n",
      "0 500 0.23012623190879822\n",
      "0 550 0.5425283908843994\n",
      "0 600 0.37102505564689636\n",
      "0 650 0.4762987494468689\n",
      "0 700 0.6412798762321472\n",
      "0 750 0.48525160551071167\n",
      "Validation loss: 0.3915623403976069 RMSE: 0.6257494\n",
      "1 29 1.0123625993728638\n",
      "1 79 0.37678879499435425\n",
      "1 129 0.30398982763290405\n",
      "1 179 0.32420504093170166\n",
      "1 229 0.6234838962554932\n",
      "1 279 0.28114068508148193\n",
      "1 329 0.24582572281360626\n",
      "1 379 0.9698102474212646\n",
      "1 429 0.2328423261642456\n",
      "1 479 0.29304569959640503\n",
      "1 529 0.4969208538532257\n",
      "1 579 0.21544530987739563\n",
      "1 629 0.4409680664539337\n",
      "1 679 0.9876284003257751\n",
      "1 729 0.2950998544692993\n",
      "Validation loss: 0.38254063707940716 RMSE: 0.6184987\n",
      "2 8 0.47384965419769287\n",
      "2 58 0.3575792908668518\n",
      "2 108 0.349895179271698\n",
      "2 158 0.5539724230766296\n",
      "2 208 0.4053380489349365\n",
      "2 258 0.8666878938674927\n",
      "2 308 0.44266507029533386\n",
      "2 358 0.275640070438385\n",
      "2 408 0.25073903799057007\n",
      "2 458 0.32953763008117676\n",
      "2 508 0.40242645144462585\n",
      "2 558 0.3604755401611328\n",
      "2 608 0.5067409873008728\n",
      "2 658 0.4238077998161316\n",
      "2 708 0.21928760409355164\n",
      "2 758 0.3293137848377228\n",
      "Validation loss: 0.4110384501297684 RMSE: 0.64112276\n",
      "3 37 0.28444549441337585\n",
      "3 87 0.13237959146499634\n",
      "3 137 0.8778683543205261\n",
      "3 187 0.1976340413093567\n",
      "3 237 0.41966140270233154\n",
      "3 287 0.21055065095424652\n",
      "3 337 0.2736467719078064\n",
      "3 387 0.6721382141113281\n",
      "3 437 0.8678511381149292\n",
      "3 487 0.7288694381713867\n",
      "3 537 0.5411942005157471\n",
      "3 587 0.4804271459579468\n",
      "3 637 0.2396632730960846\n",
      "3 687 0.2649509310722351\n",
      "3 737 0.25922471284866333\n",
      "Validation loss: 0.38218939479659586 RMSE: 0.61821467\n",
      "4 16 0.37546098232269287\n",
      "4 66 0.17537227272987366\n",
      "4 116 0.3470669984817505\n",
      "4 166 0.31820207834243774\n",
      "4 216 0.2639416456222534\n",
      "4 266 0.3531704545021057\n",
      "4 316 0.29054856300354004\n",
      "4 366 0.239975243806839\n",
      "4 416 0.2533772885799408\n",
      "4 466 0.10049086809158325\n",
      "4 516 0.21398602426052094\n",
      "4 566 0.23488977551460266\n",
      "4 616 0.4742223620414734\n",
      "4 666 0.22595876455307007\n",
      "4 716 0.22054773569107056\n",
      "4 766 0.6230785846710205\n",
      "Validation loss: 0.3750343214282218 RMSE: 0.6124005\n",
      "5 45 0.43221354484558105\n",
      "5 95 0.19900761544704437\n",
      "5 145 0.23745647072792053\n",
      "5 195 0.3313106298446655\n",
      "5 245 0.3039342761039734\n",
      "5 295 0.42444908618927\n",
      "5 345 0.2568475604057312\n",
      "5 395 0.30727699398994446\n",
      "5 445 0.4017680585384369\n",
      "5 495 0.21070757508277893\n",
      "5 545 0.3019069731235504\n",
      "5 595 0.9811256527900696\n",
      "5 645 0.27402204275131226\n",
      "5 695 0.5146363377571106\n",
      "5 745 0.24524709582328796\n",
      "Validation loss: 0.34607001103679924 RMSE: 0.58827716\n",
      "6 24 0.2885448932647705\n",
      "6 74 0.33385416865348816\n",
      "6 124 0.3990725576877594\n",
      "6 174 0.4170796275138855\n",
      "6 224 0.20336614549160004\n",
      "6 274 0.24503490328788757\n",
      "6 324 0.35057419538497925\n",
      "6 374 0.32616448402404785\n",
      "6 424 0.21231631934642792\n",
      "6 474 0.23911860585212708\n",
      "6 524 0.44254642724990845\n",
      "6 574 0.24501311779022217\n",
      "6 624 0.25031256675720215\n",
      "6 674 0.20289291441440582\n",
      "6 724 0.2704322338104248\n",
      "Validation loss: 0.3571330565301811 RMSE: 0.5976061\n",
      "7 3 0.6701005697250366\n",
      "7 53 0.584884762763977\n",
      "7 103 0.16711822152137756\n",
      "7 153 0.3131997287273407\n",
      "7 203 0.2529488503932953\n",
      "7 253 0.26472529768943787\n",
      "7 303 0.3215838670730591\n",
      "7 353 0.40509510040283203\n",
      "7 403 0.2689841687679291\n",
      "7 453 0.3778563141822815\n",
      "7 503 0.8574241399765015\n",
      "7 553 0.31538376212120056\n",
      "7 603 0.3480590283870697\n",
      "7 653 0.41065913438796997\n",
      "7 703 0.30843260884284973\n",
      "7 753 0.5241320133209229\n",
      "Validation loss: 0.36011781532536535 RMSE: 0.60009813\n",
      "8 32 0.12706661224365234\n",
      "8 82 0.1794494241476059\n",
      "8 132 0.22069606184959412\n",
      "8 182 0.1658525913953781\n",
      "8 232 0.23975542187690735\n",
      "8 282 0.2968372106552124\n",
      "8 332 0.2522038221359253\n",
      "8 382 0.1869739592075348\n",
      "8 432 0.24678650498390198\n",
      "8 482 0.25371089577674866\n",
      "8 532 0.24268141388893127\n",
      "8 582 0.17215433716773987\n",
      "8 632 0.636345386505127\n",
      "8 682 0.3776910901069641\n",
      "8 732 0.2658076882362366\n",
      "Validation loss: 0.3711097854025224 RMSE: 0.6091878\n",
      "9 11 0.4134552776813507\n",
      "9 61 0.18583397567272186\n",
      "9 111 0.17312486469745636\n",
      "9 161 0.28449106216430664\n",
      "9 211 0.365383505821228\n",
      "9 261 0.1895504891872406\n",
      "9 311 0.3277772068977356\n",
      "9 361 0.32403063774108887\n",
      "9 411 0.21437212824821472\n",
      "9 461 0.6027071475982666\n",
      "9 511 0.502448558807373\n",
      "9 561 0.42379119992256165\n",
      "9 611 0.43885165452957153\n",
      "9 661 0.24404707551002502\n",
      "9 711 0.261788547039032\n",
      "9 761 0.2887754440307617\n",
      "Validation loss: 0.32812128216028214 RMSE: 0.5728187\n",
      "10 40 0.24652819335460663\n",
      "10 90 0.1793811321258545\n",
      "10 140 0.22345919907093048\n",
      "10 190 0.1832011342048645\n",
      "10 240 0.6069347858428955\n",
      "10 290 0.21399915218353271\n",
      "10 340 0.3487556278705597\n",
      "10 390 0.26371854543685913\n",
      "10 440 0.2239742875099182\n",
      "10 490 0.18687951564788818\n",
      "10 540 0.4826636016368866\n",
      "10 590 0.3604627251625061\n",
      "10 640 0.4628766179084778\n",
      "10 690 0.2896827459335327\n",
      "10 740 0.17893369495868683\n",
      "Validation loss: 0.3356488204615958 RMSE: 0.5793521\n",
      "11 19 0.21293756365776062\n",
      "11 69 0.19023273885250092\n",
      "11 119 0.2725425362586975\n",
      "11 169 0.20044922828674316\n",
      "11 219 0.4825156629085541\n",
      "11 269 0.35214412212371826\n",
      "11 319 0.3718321919441223\n",
      "11 369 0.2901992201805115\n",
      "11 419 0.3692731261253357\n",
      "11 469 0.27788984775543213\n",
      "11 519 0.24675238132476807\n",
      "11 569 0.22059868276119232\n",
      "11 619 0.1654292345046997\n",
      "11 669 0.5474258661270142\n",
      "11 719 0.13162736594676971\n",
      "11 769 0.4642106294631958\n",
      "Validation loss: 0.329838869654957 RMSE: 0.574316\n",
      "12 48 0.25175291299819946\n",
      "12 98 0.18097805976867676\n",
      "12 148 0.19328799843788147\n",
      "12 198 0.23318248987197876\n",
      "12 248 0.21147599816322327\n",
      "12 298 0.3518994450569153\n",
      "12 348 0.5015069246292114\n",
      "12 398 0.1720232367515564\n",
      "12 448 0.21257564425468445\n",
      "12 498 0.48196884989738464\n",
      "12 548 0.37606218457221985\n",
      "12 598 0.38413122296333313\n",
      "12 648 0.34934473037719727\n",
      "12 698 0.14700540900230408\n",
      "12 748 0.23214352130889893\n",
      "Validation loss: 0.3218092189773041 RMSE: 0.5672823\n",
      "13 27 0.1733110398054123\n",
      "13 77 0.1521487832069397\n",
      "13 127 0.23370565474033356\n",
      "13 177 0.24817484617233276\n",
      "13 227 0.1914539933204651\n",
      "13 277 0.23249772191047668\n",
      "13 327 0.4029248058795929\n",
      "13 377 0.25066137313842773\n",
      "13 427 0.1617799699306488\n",
      "13 477 0.1436382234096527\n",
      "13 527 0.24235887825489044\n",
      "13 577 0.2142692506313324\n",
      "13 627 0.205291748046875\n",
      "13 677 0.18346649408340454\n",
      "13 727 0.3927319049835205\n",
      "Validation loss: 0.32530376534251604 RMSE: 0.5703541\n",
      "14 6 0.6991256475448608\n",
      "14 56 0.23726725578308105\n",
      "14 106 0.12269870191812515\n",
      "14 156 0.23409408330917358\n",
      "14 206 0.10037969797849655\n",
      "14 256 0.3666686415672302\n",
      "14 306 0.41843053698539734\n",
      "14 356 0.6490178108215332\n",
      "14 406 0.33764737844467163\n",
      "14 456 0.3502693772315979\n",
      "14 506 0.34908509254455566\n",
      "14 556 0.15834493935108185\n",
      "14 606 0.1776031255722046\n",
      "14 656 0.2884630262851715\n",
      "14 706 0.19143493473529816\n",
      "14 756 0.19685277342796326\n",
      "Validation loss: 0.31684121631962414 RMSE: 0.5628865\n",
      "15 35 0.27580025792121887\n",
      "15 85 0.1843375861644745\n",
      "15 135 0.18794359266757965\n",
      "15 185 0.7120864391326904\n",
      "15 235 0.3520723283290863\n",
      "15 285 0.36440974473953247\n",
      "15 335 0.27809059619903564\n",
      "15 385 0.2955630421638489\n",
      "15 435 0.16093650460243225\n",
      "15 485 0.2778700590133667\n",
      "15 535 0.27731460332870483\n",
      "15 585 0.3540777564048767\n",
      "15 635 0.3098139762878418\n",
      "15 685 0.4944759011268616\n",
      "15 735 0.28959527611732483\n",
      "Validation loss: 0.34792144788319573 RMSE: 0.5898487\n",
      "16 14 0.3314896821975708\n",
      "16 64 0.5046535134315491\n",
      "16 114 0.13727392256259918\n",
      "16 164 0.4203011989593506\n",
      "16 214 0.12477950751781464\n",
      "16 264 0.30456113815307617\n",
      "16 314 0.36744868755340576\n",
      "16 364 0.19588600099086761\n",
      "16 414 0.23811331391334534\n",
      "16 464 0.24743199348449707\n",
      "16 514 0.16692480444908142\n",
      "16 564 0.29683881998062134\n",
      "16 614 0.16649597883224487\n",
      "16 664 0.2821195125579834\n",
      "16 714 0.2698538601398468\n",
      "16 764 0.14719679951667786\n",
      "Validation loss: 0.3117433545891853 RMSE: 0.55833983\n",
      "17 43 0.13818538188934326\n",
      "17 93 0.20223772525787354\n",
      "17 143 0.19521871209144592\n",
      "17 193 0.13379529118537903\n",
      "17 243 0.24615609645843506\n",
      "17 293 0.16320905089378357\n",
      "17 343 0.20286016166210175\n",
      "17 393 0.2938021421432495\n",
      "17 443 0.10696814954280853\n",
      "17 493 0.23372650146484375\n",
      "17 543 0.2845960855484009\n",
      "17 593 0.17878450453281403\n",
      "17 643 0.2123640924692154\n",
      "17 693 0.35516759753227234\n",
      "17 743 0.2366885542869568\n",
      "Validation loss: 0.3174319611314465 RMSE: 0.563411\n",
      "18 22 0.21287855505943298\n",
      "18 72 0.15050579607486725\n",
      "18 122 0.17964807152748108\n",
      "18 172 0.4429352581501007\n",
      "18 222 0.28699803352355957\n",
      "18 272 0.26150062680244446\n",
      "18 322 0.3533862829208374\n",
      "18 372 0.39903873205184937\n",
      "18 422 0.23052267730236053\n",
      "18 472 0.32117387652397156\n",
      "18 522 0.32897013425827026\n",
      "18 572 0.2639407217502594\n",
      "18 622 0.19622090458869934\n",
      "18 672 0.2600807547569275\n",
      "18 722 0.4291824698448181\n",
      "Validation loss: 0.3104447862133384 RMSE: 0.5571757\n",
      "19 1 0.3952871561050415\n",
      "19 51 0.3560766875743866\n",
      "19 101 0.2868936061859131\n",
      "19 151 0.16362012922763824\n",
      "19 201 0.231409952044487\n",
      "19 251 0.2809600830078125\n",
      "19 301 0.21855929493904114\n",
      "19 351 0.2727482318878174\n",
      "19 401 0.28041425347328186\n",
      "19 451 0.24474599957466125\n",
      "19 501 0.19972720742225647\n",
      "19 551 0.24028313159942627\n",
      "19 601 0.237015500664711\n",
      "19 651 0.15838849544525146\n",
      "19 701 0.1410781443119049\n",
      "19 751 0.18668539822101593\n",
      "Validation loss: 0.31197157332344966 RMSE: 0.55854416\n",
      "20 30 0.17877766489982605\n",
      "20 80 0.4727421700954437\n",
      "20 130 0.22477440536022186\n",
      "20 180 0.13154608011245728\n",
      "20 230 0.3672313988208771\n",
      "20 280 0.20398256182670593\n",
      "20 330 0.1736401915550232\n",
      "20 380 0.6256471872329712\n",
      "20 430 0.22296515107154846\n",
      "20 480 0.17570751905441284\n",
      "20 530 0.242445707321167\n",
      "20 580 0.25706925988197327\n",
      "20 630 0.3253309726715088\n",
      "20 680 0.3843882381916046\n",
      "20 730 0.3690595030784607\n",
      "Validation loss: 0.3340191423126003 RMSE: 0.5779439\n",
      "21 9 0.29874247312545776\n",
      "21 59 0.4508648216724396\n",
      "21 109 0.15955041348934174\n",
      "21 159 0.2871360182762146\n",
      "21 209 0.18498842418193817\n",
      "21 259 0.23869875073432922\n",
      "21 309 0.3577662408351898\n",
      "21 359 0.10584212839603424\n",
      "21 409 0.39637821912765503\n",
      "21 459 0.289034903049469\n",
      "21 509 0.39295682311058044\n",
      "21 559 0.23081837594509125\n",
      "21 609 0.18501374125480652\n",
      "21 659 0.5175986886024475\n",
      "21 709 0.16444829106330872\n",
      "21 759 0.5427061319351196\n",
      "Validation loss: 0.3012115728000508 RMSE: 0.54882747\n",
      "22 38 0.2601553797721863\n",
      "22 88 0.21173414587974548\n",
      "22 138 0.5200027823448181\n",
      "22 188 0.16502030193805695\n",
      "22 238 0.3175063133239746\n",
      "22 288 0.29207295179367065\n",
      "22 338 0.24431991577148438\n",
      "22 388 0.36563727259635925\n",
      "22 438 0.33852630853652954\n",
      "22 488 0.12771661579608917\n",
      "22 538 0.1528642475605011\n",
      "22 588 0.27253392338752747\n",
      "22 638 0.26848304271698\n",
      "22 688 0.3011576533317566\n",
      "22 738 0.28416362404823303\n",
      "Validation loss: 0.30341453795485634 RMSE: 0.5508308\n",
      "23 17 0.157030388712883\n",
      "23 67 0.38208234310150146\n",
      "23 117 0.20797619223594666\n",
      "23 167 0.24888825416564941\n",
      "23 217 0.17095814645290375\n",
      "23 267 0.3154812753200531\n",
      "23 317 0.4683440327644348\n",
      "23 367 0.30795398354530334\n",
      "23 417 0.2548640966415405\n",
      "23 467 0.31163570284843445\n",
      "23 517 0.2455613911151886\n",
      "23 567 0.542664110660553\n",
      "23 617 0.1296767145395279\n",
      "23 667 0.15738223493099213\n",
      "23 717 0.3314761221408844\n",
      "23 767 0.2521818280220032\n",
      "Validation loss: 0.3168761869892478 RMSE: 0.56291753\n",
      "24 46 0.45251524448394775\n",
      "24 96 0.5008125305175781\n",
      "24 146 0.2264803797006607\n",
      "24 196 0.14443093538284302\n",
      "24 246 0.269848108291626\n",
      "24 296 0.20589450001716614\n",
      "24 346 0.28591489791870117\n",
      "24 396 0.19475534558296204\n",
      "24 446 0.24906432628631592\n",
      "24 496 0.3681558072566986\n",
      "24 546 0.14097440242767334\n",
      "24 596 0.42021235823631287\n",
      "24 646 0.19769933819770813\n",
      "24 696 0.28509587049484253\n",
      "24 746 0.18710847198963165\n",
      "Validation loss: 0.3061231312287204 RMSE: 0.55328393\n",
      "25 25 0.3345435857772827\n",
      "25 75 0.191346675157547\n",
      "25 125 0.27963370084762573\n",
      "25 175 0.21827204525470734\n",
      "25 225 0.28454023599624634\n",
      "25 275 0.1454559862613678\n",
      "25 325 0.1308496743440628\n",
      "25 375 0.23524488508701324\n",
      "25 425 0.34408658742904663\n",
      "25 475 0.18328647315502167\n",
      "25 525 0.44587311148643494\n",
      "25 575 0.31040555238723755\n",
      "25 625 0.18040528893470764\n",
      "25 675 0.157005175948143\n",
      "25 725 0.175962895154953\n",
      "Validation loss: 0.3023032582748462 RMSE: 0.54982114\n",
      "26 4 0.19139041006565094\n",
      "26 54 0.2540769577026367\n",
      "26 104 0.19673867523670197\n",
      "26 154 0.11937850713729858\n",
      "26 204 0.1332026720046997\n",
      "26 254 0.1591758131980896\n",
      "26 304 0.13400644063949585\n",
      "26 354 0.1389315128326416\n",
      "26 404 0.29046881198883057\n",
      "26 454 0.22788888216018677\n",
      "26 504 0.22256940603256226\n",
      "26 554 0.3252403438091278\n",
      "26 604 0.19788441061973572\n",
      "26 654 0.11970975995063782\n",
      "26 704 0.2839867174625397\n",
      "26 754 0.37036117911338806\n",
      "Validation loss: 0.3144650224815397 RMSE: 0.5607718\n",
      "27 33 0.36064720153808594\n",
      "27 83 0.16623598337173462\n",
      "27 133 0.1960916817188263\n",
      "27 183 0.254251629114151\n",
      "27 233 0.24362680315971375\n",
      "27 283 0.22796356678009033\n",
      "27 333 0.30964332818984985\n",
      "27 383 0.3340436816215515\n",
      "27 433 0.30034586787223816\n",
      "27 483 0.3119688034057617\n",
      "27 533 0.269112765789032\n",
      "27 583 0.2749176323413849\n",
      "27 633 0.15248478949069977\n",
      "27 683 0.3421083092689514\n",
      "27 733 0.17305710911750793\n",
      "Validation loss: 0.29955182635389704 RMSE: 0.5473133\n",
      "28 12 0.16105592250823975\n",
      "28 62 0.14325538277626038\n",
      "28 112 0.09952915459871292\n",
      "28 162 0.7724743485450745\n",
      "28 212 0.17443981766700745\n",
      "28 262 0.2971121668815613\n",
      "28 312 0.3025645315647125\n",
      "28 362 0.2505894899368286\n",
      "28 412 0.2148180902004242\n",
      "28 462 0.3625216484069824\n",
      "28 512 0.22109216451644897\n",
      "28 562 0.20925197005271912\n",
      "28 612 0.1847294270992279\n",
      "28 662 0.28874993324279785\n",
      "28 712 0.2214040458202362\n",
      "28 762 0.30805057287216187\n",
      "Validation loss: 0.2985292626544833 RMSE: 0.5463783\n",
      "29 41 0.11048081517219543\n",
      "29 91 0.383948415517807\n",
      "29 141 0.3089064657688141\n",
      "29 191 0.25849416851997375\n",
      "29 241 0.5065598487854004\n",
      "29 291 0.38874614238739014\n",
      "29 341 0.32152870297431946\n",
      "29 391 0.24072064459323883\n",
      "29 441 0.24399349093437195\n",
      "29 491 0.18023043870925903\n",
      "29 541 0.24116209149360657\n",
      "29 591 0.23253385722637177\n",
      "29 641 0.3470431864261627\n",
      "29 691 0.202501580119133\n",
      "29 741 0.2551504373550415\n",
      "Validation loss: 0.2881187368732165 RMSE: 0.5367669\n",
      "Loaded trained model with success.\n",
      "Test loss: 0.3109240378678124 Test RMSE: 0.5576056\n",
      "Running on: cuda:0\n",
      "Loaded pre-trained model with success.\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 10.055206298828125\n",
      "0 50 0.49062567949295044\n",
      "0 100 0.28468677401542664\n",
      "0 150 0.33698880672454834\n",
      "0 200 0.7592262029647827\n",
      "0 250 0.6613893508911133\n",
      "0 300 0.5486131310462952\n",
      "0 350 0.604448139667511\n",
      "0 400 0.2693502604961395\n",
      "0 450 0.43273934721946716\n",
      "0 500 0.2424447387456894\n",
      "0 550 0.2785896062850952\n",
      "0 600 0.3199775815010071\n",
      "0 650 0.5839828252792358\n",
      "0 700 0.23877748847007751\n",
      "0 750 0.3508709669113159\n",
      "Validation loss: 0.4498567059636116 RMSE: 0.6707136\n",
      "1 29 0.12971855700016022\n",
      "1 79 0.4883769750595093\n",
      "1 129 0.2814956605434418\n",
      "1 179 0.32356518507003784\n",
      "1 229 0.38203710317611694\n",
      "1 279 0.3856469392776489\n",
      "1 329 0.3257060647010803\n",
      "1 379 0.5401336550712585\n",
      "1 429 0.5712629556655884\n",
      "1 479 0.4521862268447876\n",
      "1 529 0.23064929246902466\n",
      "1 579 0.7751532196998596\n",
      "1 629 0.28598153591156006\n",
      "1 679 0.1611773669719696\n",
      "1 729 0.29398345947265625\n",
      "Validation loss: 0.4535034821533105 RMSE: 0.6734266\n",
      "2 8 0.31499186158180237\n",
      "2 58 0.4339127540588379\n",
      "2 108 0.16529777646064758\n",
      "2 158 0.8198491334915161\n",
      "2 208 0.31319499015808105\n",
      "2 258 0.5406742691993713\n",
      "2 308 0.3834472596645355\n",
      "2 358 0.4412621855735779\n",
      "2 408 0.41181135177612305\n",
      "2 458 0.8131877183914185\n",
      "2 508 0.4633448123931885\n",
      "2 558 0.15119698643684387\n",
      "2 608 0.60866379737854\n",
      "2 658 0.2984013557434082\n",
      "2 708 0.6642682552337646\n",
      "2 758 0.7815983891487122\n",
      "Validation loss: 0.4248072764671901 RMSE: 0.65177244\n",
      "3 37 0.43355703353881836\n",
      "3 87 0.6873385310173035\n",
      "3 137 0.18168306350708008\n",
      "3 187 0.21147629618644714\n",
      "3 237 0.508872926235199\n",
      "3 287 0.20157594978809357\n",
      "3 337 0.5017117857933044\n",
      "3 387 0.19862781465053558\n",
      "3 437 0.4112323820590973\n",
      "3 487 0.31103259325027466\n",
      "3 537 0.20458106696605682\n",
      "3 587 0.2451293170452118\n",
      "3 637 0.4149225950241089\n",
      "3 687 0.4238770008087158\n",
      "3 737 0.3156197667121887\n",
      "Validation loss: 0.5256369409753996 RMSE: 0.7250083\n",
      "4 16 0.1641308069229126\n",
      "4 66 0.7372233271598816\n",
      "4 116 0.7486103773117065\n",
      "4 166 0.2387876808643341\n",
      "4 216 0.18676401674747467\n",
      "4 266 0.577590823173523\n",
      "4 316 0.39130979776382446\n",
      "4 366 0.5452563166618347\n",
      "4 416 0.3541301488876343\n",
      "4 466 0.434286892414093\n",
      "4 516 0.4128769338130951\n",
      "4 566 0.16920381784439087\n",
      "4 616 0.1797785460948944\n",
      "4 666 0.24838319420814514\n",
      "4 716 0.8401269316673279\n",
      "4 766 0.318994402885437\n",
      "Validation loss: 0.46865187092300725 RMSE: 0.6845815\n",
      "5 45 0.20217952132225037\n",
      "5 95 0.5285654067993164\n",
      "5 145 0.33923831582069397\n",
      "5 195 0.4248819649219513\n",
      "5 245 0.25800248980522156\n",
      "5 295 0.29749077558517456\n",
      "5 345 0.29995304346084595\n",
      "5 395 0.4057719111442566\n",
      "5 445 0.12874135375022888\n",
      "5 495 0.3038851022720337\n",
      "5 545 0.4459635019302368\n",
      "5 595 0.4029366374015808\n",
      "5 645 0.44315290451049805\n",
      "5 695 0.47266751527786255\n",
      "5 745 0.14443659782409668\n",
      "Validation loss: 0.4416494614389889 RMSE: 0.6645671\n",
      "6 24 0.18786244094371796\n",
      "6 74 0.4019739031791687\n",
      "6 124 0.2420496791601181\n",
      "6 174 0.22299937903881073\n",
      "6 224 0.286691278219223\n",
      "6 274 0.8968350291252136\n",
      "6 324 0.4759580194950104\n",
      "6 374 0.4627224802970886\n",
      "6 424 0.5388669967651367\n",
      "6 474 0.6084655523300171\n",
      "6 524 0.15654969215393066\n",
      "6 574 0.26793578267097473\n",
      "6 624 0.2878524363040924\n",
      "6 674 0.263465940952301\n",
      "6 724 0.48260146379470825\n",
      "Validation loss: 0.45836804609964876 RMSE: 0.67702883\n",
      "7 3 0.09206918627023697\n",
      "7 53 0.2289460003376007\n",
      "7 103 0.23733171820640564\n",
      "7 153 0.2656755745410919\n",
      "7 203 0.3156818747520447\n",
      "7 253 0.2625479996204376\n",
      "7 303 0.5424144268035889\n",
      "7 353 0.19891561567783356\n",
      "7 403 0.13699375092983246\n",
      "7 453 0.21657177805900574\n",
      "7 503 0.21321192383766174\n",
      "7 553 0.24950635433197021\n",
      "7 603 0.2783951759338379\n",
      "7 653 0.449096143245697\n",
      "7 703 0.5146090984344482\n",
      "7 753 0.2585080564022064\n",
      "Validation loss: 0.4200778852271683 RMSE: 0.6481342\n",
      "8 32 0.29668623208999634\n",
      "8 82 0.7056009769439697\n",
      "8 132 0.3560986816883087\n",
      "8 182 0.18958163261413574\n",
      "8 232 0.448793888092041\n",
      "8 282 0.5480661392211914\n",
      "8 332 0.36241328716278076\n",
      "8 382 0.28598421812057495\n",
      "8 432 0.15833497047424316\n",
      "8 482 0.3985491693019867\n",
      "8 532 0.18692399561405182\n",
      "8 582 0.4343753159046173\n",
      "8 632 0.7813290953636169\n",
      "8 682 0.20757970213890076\n",
      "8 732 0.21253849565982819\n",
      "Validation loss: 0.370295957169112 RMSE: 0.6085195\n",
      "9 11 0.32082319259643555\n",
      "9 61 0.22287188470363617\n",
      "9 111 0.4079221189022064\n",
      "9 161 0.23765361309051514\n",
      "9 211 0.2506692111492157\n",
      "9 261 0.35182857513427734\n",
      "9 311 0.3344157338142395\n",
      "9 361 0.2791549265384674\n",
      "9 411 0.5180307626724243\n",
      "9 461 0.2093195617198944\n",
      "9 511 0.4278032183647156\n",
      "9 561 0.2325853854417801\n",
      "9 611 0.17015768587589264\n",
      "9 661 0.4775903820991516\n",
      "9 711 0.31592756509780884\n",
      "9 761 0.22866320610046387\n",
      "Validation loss: 0.37162511269835863 RMSE: 0.6096106\n",
      "10 40 0.35923564434051514\n",
      "10 90 0.26888057589530945\n",
      "10 140 0.17771005630493164\n",
      "10 190 0.34795889258384705\n",
      "10 240 0.22250473499298096\n",
      "10 290 0.4401302933692932\n",
      "10 340 0.3356437087059021\n",
      "10 390 0.2881895899772644\n",
      "10 440 0.14095883071422577\n",
      "10 490 0.3364124894142151\n",
      "10 540 0.30070602893829346\n",
      "10 590 0.26853466033935547\n",
      "10 640 0.38856270909309387\n",
      "10 690 0.2760300636291504\n",
      "10 740 0.21823924779891968\n",
      "Validation loss: 0.4149998175802038 RMSE: 0.6442048\n",
      "11 19 0.8149022459983826\n",
      "11 69 0.3240613341331482\n",
      "11 119 0.2337559461593628\n",
      "11 169 0.1911972463130951\n",
      "11 219 0.7116084098815918\n",
      "11 269 0.29601579904556274\n",
      "11 319 0.5057733058929443\n",
      "11 369 0.20720630884170532\n",
      "11 419 0.44227179884910583\n",
      "11 469 0.29410403966903687\n",
      "11 519 0.16338501870632172\n",
      "11 569 0.17655552923679352\n",
      "11 619 0.19679862260818481\n",
      "11 669 0.2464701235294342\n",
      "11 719 0.2138405442237854\n",
      "11 769 0.35742613673210144\n",
      "Validation loss: 0.3696346479084562 RMSE: 0.60797584\n",
      "12 48 0.14674639701843262\n",
      "12 98 0.23632465302944183\n",
      "12 148 0.3747304677963257\n",
      "12 198 0.8152359127998352\n",
      "12 248 0.4383973479270935\n",
      "12 298 0.26048773527145386\n",
      "12 348 0.21775756776332855\n",
      "12 398 0.2522684335708618\n",
      "12 448 0.08449708670377731\n",
      "12 498 0.780052661895752\n",
      "12 548 0.11262458562850952\n",
      "12 598 0.24258533120155334\n",
      "12 648 0.3161894381046295\n",
      "12 698 0.11777990311384201\n",
      "12 748 0.3085259199142456\n",
      "Validation loss: 0.37571417995016365 RMSE: 0.6129553\n",
      "13 27 0.1410447657108307\n",
      "13 77 0.5464189052581787\n",
      "13 127 0.2408086061477661\n",
      "13 177 0.49351930618286133\n",
      "13 227 0.4610808789730072\n",
      "13 277 0.2406085729598999\n",
      "13 327 0.27829709649086\n",
      "13 377 0.40783408284187317\n",
      "13 427 0.253958523273468\n",
      "13 477 0.46816909313201904\n",
      "13 527 0.14967162907123566\n",
      "13 577 0.21343925595283508\n",
      "13 627 0.2306835651397705\n",
      "13 677 0.2982999086380005\n",
      "13 727 0.3067188858985901\n",
      "Validation loss: 0.3533653439953923 RMSE: 0.5944454\n",
      "14 6 0.11864396929740906\n",
      "14 56 0.16102176904678345\n",
      "14 106 0.43467438220977783\n",
      "14 156 0.23236863315105438\n",
      "14 206 0.1349039077758789\n",
      "14 256 0.40701907873153687\n",
      "14 306 0.330471396446228\n",
      "14 356 0.27053946256637573\n",
      "14 406 0.2263311892747879\n",
      "14 456 0.45368778705596924\n",
      "14 506 0.3516332805156708\n",
      "14 556 0.13903725147247314\n",
      "14 606 0.2321835607290268\n",
      "14 656 0.253865510225296\n",
      "14 706 0.25032132863998413\n",
      "14 756 0.09096016734838486\n",
      "Validation loss: 0.38320320767953114 RMSE: 0.6190341\n",
      "15 35 0.3415069580078125\n",
      "15 85 0.31862038373947144\n",
      "15 135 0.15300816297531128\n",
      "15 185 0.23329758644104004\n",
      "15 235 0.3078218400478363\n",
      "15 285 0.3055140972137451\n",
      "15 335 0.22826337814331055\n",
      "15 385 0.30804380774497986\n",
      "15 435 0.35226356983184814\n",
      "15 485 0.2804829180240631\n",
      "15 535 0.24804872274398804\n",
      "15 585 0.4464781880378723\n",
      "15 635 0.28892362117767334\n",
      "15 685 0.3375674784183502\n",
      "15 735 0.38067877292633057\n",
      "Validation loss: 0.3281325468355242 RMSE: 0.57282853\n",
      "16 14 0.3273225426673889\n",
      "16 64 0.3100442886352539\n",
      "16 114 0.3091701865196228\n",
      "16 164 0.1628253012895584\n",
      "16 214 0.23404642939567566\n",
      "16 264 0.19521759450435638\n",
      "16 314 0.12722332775592804\n",
      "16 364 0.14091627299785614\n",
      "16 414 0.13703645765781403\n",
      "16 464 0.1721101850271225\n",
      "16 514 0.3346163034439087\n",
      "16 564 0.14598704874515533\n",
      "16 614 0.2099183201789856\n",
      "16 664 0.5328713655471802\n",
      "16 714 0.22509929537773132\n",
      "16 764 0.3443062901496887\n",
      "Validation loss: 0.3574205372372971 RMSE: 0.59784657\n",
      "17 43 0.11916178464889526\n",
      "17 93 0.23553696274757385\n",
      "17 143 0.3350796699523926\n",
      "17 193 0.2707468867301941\n",
      "17 243 0.1655236780643463\n",
      "17 293 0.19798243045806885\n",
      "17 343 0.14550110697746277\n",
      "17 393 0.2850770950317383\n",
      "17 443 0.3072296679019928\n",
      "17 493 0.18899187445640564\n",
      "17 543 0.3158789873123169\n",
      "17 593 0.17382775247097015\n",
      "17 643 0.19856971502304077\n",
      "17 693 0.1772787868976593\n",
      "17 743 0.44212132692337036\n",
      "Validation loss: 0.3399911819245009 RMSE: 0.5830876\n",
      "18 22 0.18181878328323364\n",
      "18 72 0.13780418038368225\n",
      "18 122 0.5170443654060364\n",
      "18 172 0.3196864426136017\n",
      "18 222 0.3283439874649048\n",
      "18 272 0.4507662355899811\n",
      "18 322 0.29003003239631653\n",
      "18 372 0.46519917249679565\n",
      "18 422 0.3095681965351105\n",
      "18 472 0.14135493338108063\n",
      "18 522 0.8095368146896362\n",
      "18 572 0.4585879445075989\n",
      "18 622 0.4088539779186249\n",
      "18 672 0.22012895345687866\n",
      "18 722 0.17517009377479553\n",
      "Validation loss: 0.3311597239226103 RMSE: 0.5754648\n",
      "19 1 0.3545718789100647\n",
      "19 51 0.13628576695919037\n",
      "19 101 0.2779940366744995\n",
      "19 151 0.264926940202713\n",
      "19 201 0.13457337021827698\n",
      "19 251 0.21190303564071655\n",
      "19 301 0.36665078997612\n",
      "19 351 0.18841123580932617\n",
      "19 401 0.39393073320388794\n",
      "19 451 0.18649829924106598\n",
      "19 501 0.27313321828842163\n",
      "19 551 0.18544192612171173\n",
      "19 601 0.1917247176170349\n",
      "19 651 0.1970808207988739\n",
      "19 701 0.3097599148750305\n",
      "19 751 0.3190711736679077\n",
      "Validation loss: 0.336098673479522 RMSE: 0.57974017\n",
      "20 30 0.2689228057861328\n",
      "20 80 0.20235204696655273\n",
      "20 130 0.30547773838043213\n",
      "20 180 0.1749541163444519\n",
      "20 230 0.23628684878349304\n",
      "20 280 0.19664746522903442\n",
      "20 330 0.22053839266300201\n",
      "20 380 0.21883463859558105\n",
      "20 430 0.14334736764431\n",
      "20 480 0.21788987517356873\n",
      "20 530 0.2451428472995758\n",
      "20 580 0.5494456887245178\n",
      "20 630 0.16657215356826782\n",
      "20 680 0.2593854069709778\n",
      "20 730 0.2935641407966614\n",
      "Validation loss: 0.3103691934991409 RMSE: 0.55710787\n",
      "21 9 0.3806309998035431\n",
      "21 59 0.20288896560668945\n",
      "21 109 0.285691499710083\n",
      "21 159 0.290688693523407\n",
      "21 209 0.21817190945148468\n",
      "21 259 0.20464107394218445\n",
      "21 309 0.31153610348701477\n",
      "21 359 0.3285207152366638\n",
      "21 409 0.12216290831565857\n",
      "21 459 0.13789355754852295\n",
      "21 509 0.09237012267112732\n",
      "21 559 0.1476883888244629\n",
      "21 609 0.27607786655426025\n",
      "21 659 0.17630213499069214\n",
      "21 709 0.16698428988456726\n",
      "21 759 0.43099454045295715\n",
      "Validation loss: 0.3061730796471238 RMSE: 0.5533291\n",
      "22 38 0.1809854805469513\n",
      "22 88 0.0826047733426094\n",
      "22 138 0.11562994867563248\n",
      "22 188 0.236533984541893\n",
      "22 238 0.13402284681797028\n",
      "22 288 0.33422166109085083\n",
      "22 338 0.24560408294200897\n",
      "22 388 0.293317973613739\n",
      "22 438 0.11617451906204224\n",
      "22 488 0.4799148142337799\n",
      "22 538 0.24568887054920197\n",
      "22 588 0.22125300765037537\n",
      "22 638 0.6173503398895264\n",
      "22 688 0.1293666809797287\n",
      "22 738 0.5795524716377258\n",
      "Validation loss: 0.3001002436503768 RMSE: 0.5478141\n",
      "23 17 0.15469369292259216\n",
      "23 67 0.3026342988014221\n",
      "23 117 0.15387922525405884\n",
      "23 167 0.270675390958786\n",
      "23 217 0.3817644715309143\n",
      "23 267 0.234602153301239\n",
      "23 317 0.23816978931427002\n",
      "23 367 0.347261518239975\n",
      "23 417 0.23097804188728333\n",
      "23 467 0.18327024579048157\n",
      "23 517 0.3293900787830353\n",
      "23 567 0.198004812002182\n",
      "23 617 0.2857509255409241\n",
      "23 667 0.19832904636859894\n",
      "23 717 0.2640460431575775\n",
      "23 767 0.22698739171028137\n",
      "Validation loss: 0.3048614357762477 RMSE: 0.55214256\n",
      "24 46 0.23664942383766174\n",
      "24 96 0.1982685774564743\n",
      "24 146 0.32079750299453735\n",
      "24 196 0.22121073305606842\n",
      "24 246 0.2789924144744873\n",
      "24 296 0.16296075284481049\n",
      "24 346 0.32307082414627075\n",
      "24 396 0.2445005178451538\n",
      "24 446 0.13138441741466522\n",
      "24 496 0.38214796781539917\n",
      "24 546 0.2055664360523224\n",
      "24 596 0.2694794535636902\n",
      "24 646 0.25488245487213135\n",
      "24 696 0.18593665957450867\n",
      "24 746 0.1261071264743805\n",
      "Validation loss: 0.3244428429752588 RMSE: 0.56959885\n",
      "25 25 0.107313372194767\n",
      "25 75 0.2970787286758423\n",
      "25 125 0.471355676651001\n",
      "25 175 0.1744852066040039\n",
      "25 225 0.4447035789489746\n",
      "25 275 0.23623423278331757\n",
      "25 325 0.12293532490730286\n",
      "25 375 0.18051835894584656\n",
      "25 425 0.12278127670288086\n",
      "25 475 0.19062775373458862\n",
      "25 525 0.24819017946720123\n",
      "25 575 0.19482779502868652\n",
      "25 625 0.9240583777427673\n",
      "25 675 0.1498868763446808\n",
      "25 725 0.06333358585834503\n",
      "Validation loss: 0.31047394953887253 RMSE: 0.5572019\n",
      "26 4 0.14821460843086243\n",
      "26 54 0.2281888723373413\n",
      "26 104 0.3423739969730377\n",
      "26 154 0.199722021818161\n",
      "26 204 0.2854801416397095\n",
      "26 254 0.3059993386268616\n",
      "26 304 0.366296648979187\n",
      "26 354 0.09279029816389084\n",
      "26 404 0.2891324460506439\n",
      "26 454 0.15448717772960663\n",
      "26 504 0.22415658831596375\n",
      "26 554 0.19154134392738342\n",
      "26 604 0.21408361196517944\n",
      "26 654 0.2698226273059845\n",
      "26 704 0.28617364168167114\n",
      "26 754 0.22506022453308105\n",
      "Validation loss: 0.2983776903985178 RMSE: 0.5462396\n",
      "27 33 0.3821338415145874\n",
      "27 83 0.2748870849609375\n",
      "27 133 0.1816921979188919\n",
      "27 183 0.1751249134540558\n",
      "27 233 0.13520433008670807\n",
      "27 283 0.2448575794696808\n",
      "27 333 0.2813919186592102\n",
      "27 383 0.19577889144420624\n",
      "27 433 0.14991746842861176\n",
      "27 483 0.10072275996208191\n",
      "27 533 0.339494526386261\n",
      "27 583 0.19336119294166565\n",
      "27 633 0.1310526430606842\n",
      "27 683 0.202243834733963\n",
      "27 733 0.4324997365474701\n",
      "Validation loss: 0.29369484446942806 RMSE: 0.54193616\n",
      "28 12 0.3555011451244354\n",
      "28 62 0.1732107549905777\n",
      "28 112 0.16339080035686493\n",
      "28 162 0.19254830479621887\n",
      "28 212 0.2656914293766022\n",
      "28 262 0.31883639097213745\n",
      "28 312 0.28678184747695923\n",
      "28 362 0.3409353494644165\n",
      "28 412 0.18736562132835388\n",
      "28 462 0.2731984555721283\n",
      "28 512 0.1991390585899353\n",
      "28 562 0.284079372882843\n",
      "28 612 0.6262946128845215\n",
      "28 662 0.22738437354564667\n",
      "28 712 0.1946401447057724\n",
      "28 762 0.2859850823879242\n",
      "Validation loss: 0.3140985980520354 RMSE: 0.560445\n",
      "29 41 0.21127034723758698\n",
      "29 91 0.11431469023227692\n",
      "29 141 0.20789805054664612\n",
      "29 191 0.24880582094192505\n",
      "29 241 0.24375025928020477\n",
      "29 291 0.22512255609035492\n",
      "29 341 0.19776953756809235\n",
      "29 391 0.1146232858300209\n",
      "29 441 0.43370598554611206\n",
      "29 491 0.24604815244674683\n",
      "29 541 0.2508113384246826\n",
      "29 591 0.1651701182126999\n",
      "29 641 0.0956670492887497\n",
      "29 691 0.20803074538707733\n",
      "29 741 0.2663397789001465\n",
      "Validation loss: 0.2966164323117803 RMSE: 0.54462504\n",
      "Loaded trained model with success.\n",
      "Test loss: 0.26653974943493625 Test RMSE: 0.51627487\n",
      "Running on: cuda:0\n",
      "Loaded pre-trained model with success.\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 11.517084121704102\n",
      "0 50 0.5215551257133484\n",
      "0 100 0.4670603573322296\n",
      "0 150 0.41933491826057434\n",
      "0 200 0.266128271818161\n",
      "0 250 0.30172377824783325\n",
      "0 300 0.2346130609512329\n",
      "0 350 0.31252673268318176\n",
      "0 400 0.8666313886642456\n",
      "0 450 0.3378713130950928\n",
      "0 500 0.49326109886169434\n",
      "0 550 0.281850665807724\n",
      "0 600 0.37626034021377563\n",
      "0 650 0.6664769649505615\n",
      "0 700 0.5406662225723267\n",
      "0 750 0.34361913800239563\n",
      "Validation loss: 0.36278581400127974 RMSE: 0.60231704\n",
      "1 29 0.28228944540023804\n",
      "1 79 0.6091058254241943\n",
      "1 129 0.5732154846191406\n",
      "1 179 0.20275229215621948\n",
      "1 229 0.7358517646789551\n",
      "1 279 0.5869460701942444\n",
      "1 329 0.5098927021026611\n",
      "1 379 0.41654008626937866\n",
      "1 429 0.2670964002609253\n",
      "1 479 0.22368085384368896\n",
      "1 529 0.382432222366333\n",
      "1 579 0.500777542591095\n",
      "1 629 0.291603684425354\n",
      "1 679 0.24800339341163635\n",
      "1 729 0.8380952477455139\n",
      "Validation loss: 0.3483466881820384 RMSE: 0.590209\n",
      "2 8 0.2846648693084717\n",
      "2 58 0.3255807161331177\n",
      "2 108 0.44701552391052246\n",
      "2 158 0.5667703151702881\n",
      "2 208 0.3101091682910919\n",
      "2 258 0.4155086278915405\n",
      "2 308 0.7151164412498474\n",
      "2 358 0.37649691104888916\n",
      "2 408 0.3921304941177368\n",
      "2 458 0.1909213364124298\n",
      "2 508 0.17683175206184387\n",
      "2 558 0.1832260638475418\n",
      "2 608 0.3109142780303955\n",
      "2 658 0.1895892173051834\n",
      "2 708 0.37737298011779785\n",
      "2 758 0.22324378788471222\n",
      "Validation loss: 0.35167517523993463 RMSE: 0.5930221\n",
      "3 37 0.39681708812713623\n",
      "3 87 0.3527812659740448\n",
      "3 137 0.4279783368110657\n",
      "3 187 0.484993577003479\n",
      "3 237 0.670093297958374\n",
      "3 287 0.17692157626152039\n",
      "3 337 0.5251214504241943\n",
      "3 387 0.3922749161720276\n",
      "3 437 0.43661969900131226\n",
      "3 487 0.4552958607673645\n",
      "3 537 0.38817298412323\n",
      "3 587 0.24167945981025696\n",
      "3 637 0.1804853081703186\n",
      "3 687 0.2879188060760498\n",
      "3 737 0.23512780666351318\n",
      "Validation loss: 0.3224546069498448 RMSE: 0.5678509\n",
      "4 16 0.49186575412750244\n",
      "4 66 0.3588380813598633\n",
      "4 116 1.066582202911377\n",
      "4 166 0.38547444343566895\n",
      "4 216 0.3505428731441498\n",
      "4 266 0.3277221918106079\n",
      "4 316 0.3139396011829376\n",
      "4 366 0.18898333609104156\n",
      "4 416 0.3716992735862732\n",
      "4 466 0.2971518635749817\n",
      "4 516 0.5835297107696533\n",
      "4 566 0.22791171073913574\n",
      "4 616 0.2988590598106384\n",
      "4 666 0.2933982312679291\n",
      "4 716 0.35373756289482117\n",
      "4 766 0.3287680149078369\n",
      "Validation loss: 0.3355989905402941 RMSE: 0.57930905\n",
      "5 45 0.28751450777053833\n",
      "5 95 0.5942626595497131\n",
      "5 145 0.2068471610546112\n",
      "5 195 0.59724360704422\n",
      "5 245 0.20479229092597961\n",
      "5 295 0.2739899158477783\n",
      "5 345 0.37471288442611694\n",
      "5 395 0.4728250205516815\n",
      "5 445 0.3246729373931885\n",
      "5 495 0.339000403881073\n",
      "5 545 0.47539547085762024\n",
      "5 595 0.2583356499671936\n",
      "5 645 0.1573573350906372\n",
      "5 695 0.5491602420806885\n",
      "5 745 0.20951783657073975\n",
      "Validation loss: 0.31714091192492666 RMSE: 0.5631527\n",
      "6 24 0.35265040397644043\n",
      "6 74 0.28463393449783325\n",
      "6 124 0.35494256019592285\n",
      "6 174 0.3188759684562683\n",
      "6 224 0.19572484493255615\n",
      "6 274 0.2777194380760193\n",
      "6 324 0.3711727261543274\n",
      "6 374 0.4525507092475891\n",
      "6 424 0.3227531909942627\n",
      "6 474 0.2734362781047821\n",
      "6 524 0.36564013361930847\n",
      "6 574 0.3140628933906555\n",
      "6 624 0.5636066794395447\n",
      "6 674 0.37312203645706177\n",
      "6 724 0.29076921939849854\n",
      "Validation loss: 0.34494345021598477 RMSE: 0.58731884\n",
      "7 3 0.5643476247787476\n",
      "7 53 0.1418738216161728\n",
      "7 103 0.18324831128120422\n",
      "7 153 0.37584805488586426\n",
      "7 203 0.2096964418888092\n",
      "7 253 0.20043101906776428\n",
      "7 303 0.2978993058204651\n",
      "7 353 0.7279085516929626\n",
      "7 403 0.24211490154266357\n",
      "7 453 0.36521387100219727\n",
      "7 503 0.3013624846935272\n",
      "7 553 0.24967029690742493\n",
      "7 603 0.3688576817512512\n",
      "7 653 0.3874995708465576\n",
      "7 703 0.29435741901397705\n",
      "7 753 0.40481066703796387\n",
      "Validation loss: 0.2932318557492074 RMSE: 0.54150885\n",
      "8 32 0.42891111969947815\n",
      "8 82 0.29586559534072876\n",
      "8 132 0.21034355461597443\n",
      "8 182 0.25675785541534424\n",
      "8 232 0.22547204792499542\n",
      "8 282 0.23336222767829895\n",
      "8 332 0.2958407402038574\n",
      "8 382 0.5490379333496094\n",
      "8 432 0.21527008712291718\n",
      "8 482 0.3057001233100891\n",
      "8 532 0.2664221525192261\n",
      "8 582 0.7085558176040649\n",
      "8 632 0.24723124504089355\n",
      "8 682 0.22679109871387482\n",
      "8 732 0.9732687473297119\n",
      "Validation loss: 0.2988257302957423 RMSE: 0.5466495\n",
      "9 11 0.24204707145690918\n",
      "9 61 0.3675708472728729\n",
      "9 111 0.7153911590576172\n",
      "9 161 0.23207703232765198\n",
      "9 211 0.2655908763408661\n",
      "9 261 0.49149370193481445\n",
      "9 311 0.3333902657032013\n",
      "9 361 0.8397101163864136\n",
      "9 411 0.8753002285957336\n",
      "9 461 0.24143435060977936\n",
      "9 511 0.28608906269073486\n",
      "9 561 0.24925559759140015\n",
      "9 611 0.2509329319000244\n",
      "9 661 0.21640680730342865\n",
      "9 711 0.2238711416721344\n",
      "9 761 0.20718660950660706\n",
      "Validation loss: 0.28540678672930775 RMSE: 0.53423476\n",
      "10 40 0.22187666594982147\n",
      "10 90 0.3893902897834778\n",
      "10 140 0.5679721832275391\n",
      "10 190 0.2703598141670227\n",
      "10 240 0.35598477721214294\n",
      "10 290 0.3296900987625122\n",
      "10 340 0.3503428101539612\n",
      "10 390 0.2534947395324707\n",
      "10 440 0.5397835969924927\n",
      "10 490 0.314353346824646\n",
      "10 540 0.7987349033355713\n",
      "10 590 0.49287447333335876\n",
      "10 640 0.5185524225234985\n",
      "10 690 0.2718397378921509\n",
      "10 740 0.33511197566986084\n",
      "Validation loss: 0.2920554141678354 RMSE: 0.5404215\n",
      "11 19 0.7609951496124268\n",
      "11 69 0.20644721388816833\n",
      "11 119 0.1609693467617035\n",
      "11 169 0.27125561237335205\n",
      "11 219 0.5298780798912048\n",
      "11 269 0.5864090323448181\n",
      "11 319 0.581932008266449\n",
      "11 369 0.3344730734825134\n",
      "11 419 0.3596721887588501\n",
      "11 469 0.20697972178459167\n",
      "11 519 0.321731835603714\n",
      "11 569 0.5793455243110657\n",
      "11 619 0.32387107610702515\n",
      "11 669 0.42051371932029724\n",
      "11 719 0.2873690724372864\n",
      "11 769 0.40355896949768066\n",
      "Validation loss: 0.28537841667147246 RMSE: 0.53420824\n",
      "12 48 0.4375513792037964\n",
      "12 98 0.23084457218647003\n",
      "12 148 0.28559744358062744\n",
      "12 198 0.21878863871097565\n",
      "12 248 0.3791576325893402\n",
      "12 298 0.21191272139549255\n",
      "12 348 0.4129599630832672\n",
      "12 398 0.37473541498184204\n",
      "12 448 0.14038759469985962\n",
      "12 498 0.23283353447914124\n",
      "12 548 0.36918091773986816\n",
      "12 598 0.5140255689620972\n",
      "12 648 0.3691932260990143\n",
      "12 698 0.26275473833084106\n",
      "12 748 0.12903741002082825\n",
      "Validation loss: 0.3053523659815683 RMSE: 0.552587\n",
      "13 27 0.27823734283447266\n",
      "13 77 0.23626847565174103\n",
      "13 127 0.5336818695068359\n",
      "13 177 0.19924461841583252\n",
      "13 227 0.335449755191803\n",
      "13 277 0.45423364639282227\n",
      "13 327 0.29275354743003845\n",
      "13 377 0.2444722056388855\n",
      "13 427 0.3336564600467682\n",
      "13 477 0.25977882742881775\n",
      "13 527 0.2794124186038971\n",
      "13 577 0.383177787065506\n",
      "13 627 0.18122121691703796\n",
      "13 677 0.48141810297966003\n",
      "13 727 0.44321322441101074\n",
      "Validation loss: 0.2941644828328315 RMSE: 0.5423693\n",
      "14 6 0.29544585943222046\n",
      "14 56 0.19543470442295074\n",
      "14 106 0.2869564890861511\n",
      "14 156 0.1582258939743042\n",
      "14 206 0.1415739357471466\n",
      "14 256 0.21284833550453186\n",
      "14 306 0.22975054383277893\n",
      "14 356 0.3862971067428589\n",
      "14 406 0.09956365823745728\n",
      "14 456 0.28202447295188904\n",
      "14 506 0.24455663561820984\n",
      "14 556 0.3938867449760437\n",
      "14 606 0.24925123155117035\n",
      "14 656 0.24607837200164795\n",
      "14 706 0.1622665822505951\n",
      "14 756 0.29519927501678467\n",
      "Validation loss: 0.285926260656732 RMSE: 0.5347207\n",
      "15 35 0.5352967977523804\n",
      "15 85 0.39814886450767517\n",
      "15 135 0.22661954164505005\n",
      "15 185 0.24987247586250305\n",
      "15 235 0.18812501430511475\n",
      "15 285 0.13124428689479828\n",
      "15 335 0.11857600510120392\n",
      "15 385 0.16041529178619385\n",
      "15 435 0.41632992029190063\n",
      "15 485 0.3276749849319458\n",
      "15 535 0.20546716451644897\n",
      "15 585 0.41233476996421814\n",
      "15 635 0.23169869184494019\n",
      "15 685 0.31938549876213074\n",
      "15 735 0.1680188626050949\n",
      "Validation loss: 0.26251158057985935 RMSE: 0.51235884\n",
      "16 14 0.2044987976551056\n",
      "16 64 0.3386475741863251\n",
      "16 114 0.1754980981349945\n",
      "16 164 0.2347964346408844\n",
      "16 214 0.3980584144592285\n",
      "16 264 0.18849049508571625\n",
      "16 314 0.29691264033317566\n",
      "16 364 0.2256803810596466\n",
      "16 414 0.2201744168996811\n",
      "16 464 0.4051517844200134\n",
      "16 514 0.32324934005737305\n",
      "16 564 0.3137023448944092\n",
      "16 614 0.20995032787322998\n",
      "16 664 0.3372526466846466\n",
      "16 714 0.2893713712692261\n",
      "16 764 0.23685598373413086\n",
      "Validation loss: 0.2689277990995085 RMSE: 0.5185825\n",
      "17 43 0.18194185197353363\n",
      "17 93 0.3700728416442871\n",
      "17 143 0.6460078358650208\n",
      "17 193 0.18203943967819214\n",
      "17 243 0.20445001125335693\n",
      "17 293 0.23332825303077698\n",
      "17 343 0.3156128525733948\n",
      "17 393 0.301250159740448\n",
      "17 443 0.3632965683937073\n",
      "17 493 0.26028093695640564\n",
      "17 543 0.2411266267299652\n",
      "17 593 0.2559313178062439\n",
      "17 643 0.3958445191383362\n",
      "17 693 0.5951083898544312\n",
      "17 743 0.3299105763435364\n",
      "Validation loss: 0.2766621093649198 RMSE: 0.5259868\n",
      "18 22 0.26847517490386963\n",
      "18 72 0.3949265778064728\n",
      "18 122 0.2326183021068573\n",
      "18 172 0.47286710143089294\n",
      "18 222 0.2259911596775055\n",
      "18 272 0.33278870582580566\n",
      "18 322 0.14062455296516418\n",
      "18 372 0.20761209726333618\n",
      "18 422 0.3842025697231293\n",
      "18 472 0.16918101906776428\n",
      "18 522 0.19336873292922974\n",
      "18 572 0.3338094651699066\n",
      "18 622 0.6655582785606384\n",
      "18 672 0.235042542219162\n",
      "18 722 0.188156396150589\n",
      "Validation loss: 0.2682690755190218 RMSE: 0.51794696\n",
      "19 1 0.44347548484802246\n",
      "19 51 0.38272589445114136\n",
      "19 101 0.3587159216403961\n",
      "19 151 0.2504020035266876\n",
      "19 201 0.20816582441329956\n",
      "19 251 0.3457835912704468\n",
      "19 301 0.42537832260131836\n",
      "19 351 0.14436902105808258\n",
      "19 401 0.42824769020080566\n",
      "19 451 0.556384265422821\n",
      "19 501 0.17151635885238647\n",
      "19 551 0.2762622833251953\n",
      "19 601 0.2559608519077301\n",
      "19 651 0.3666698932647705\n",
      "19 701 0.25480854511260986\n",
      "19 751 0.4353557229042053\n",
      "Validation loss: 0.2667270378921838 RMSE: 0.51645625\n",
      "20 30 0.32057714462280273\n",
      "20 80 0.11764431744813919\n",
      "20 130 0.14988265931606293\n",
      "20 180 0.2014380395412445\n",
      "20 230 0.2422967553138733\n",
      "20 280 0.40741342306137085\n",
      "20 330 0.19560913741588593\n",
      "20 380 0.241232767701149\n",
      "20 430 0.18399299681186676\n",
      "20 480 0.2070307433605194\n",
      "20 530 0.153935045003891\n",
      "20 580 0.23986247181892395\n",
      "20 630 0.3315463066101074\n",
      "20 680 0.18163470923900604\n",
      "20 730 0.23733380436897278\n",
      "Validation loss: 0.25778897495611625 RMSE: 0.50772923\n",
      "21 9 0.33535799384117126\n",
      "21 59 0.3145137429237366\n",
      "21 109 0.3369326591491699\n",
      "21 159 0.2776833176612854\n",
      "21 209 0.4712197184562683\n",
      "21 259 0.36507248878479004\n",
      "21 309 0.2453746348619461\n",
      "21 359 0.19272227585315704\n",
      "21 409 0.2055615484714508\n",
      "21 459 0.9383640289306641\n",
      "21 509 0.2706899642944336\n",
      "21 559 0.2025362104177475\n",
      "21 609 0.2310411036014557\n",
      "21 659 0.6152827739715576\n",
      "21 709 0.2001975178718567\n",
      "21 759 0.3629390001296997\n",
      "Validation loss: 0.2628743301200516 RMSE: 0.5127127\n",
      "22 38 0.19173216819763184\n",
      "22 88 0.4808039665222168\n",
      "22 138 0.14923164248466492\n",
      "22 188 0.18431374430656433\n",
      "22 238 0.3448426127433777\n",
      "22 288 0.22080302238464355\n",
      "22 338 0.21776534616947174\n",
      "22 388 0.22862762212753296\n",
      "22 438 0.3525901436805725\n",
      "22 488 0.07780873030424118\n",
      "22 538 0.14292004704475403\n",
      "22 588 0.10594961047172546\n",
      "22 638 0.1402285397052765\n",
      "22 688 0.2352476567029953\n",
      "22 738 0.188942551612854\n",
      "Validation loss: 0.2600519227630952 RMSE: 0.50995284\n",
      "23 17 0.4544854760169983\n",
      "23 67 0.28642532229423523\n",
      "23 117 0.25584378838539124\n",
      "23 167 0.09114150702953339\n",
      "23 217 0.1399986892938614\n",
      "23 267 0.1367424726486206\n",
      "23 317 0.23970597982406616\n",
      "23 367 0.2683137059211731\n",
      "23 417 0.5903421640396118\n",
      "23 467 0.12061597406864166\n",
      "23 517 0.3526942729949951\n",
      "23 567 0.2978578209877014\n",
      "23 617 0.10755011439323425\n",
      "23 667 0.15865828096866608\n",
      "23 717 0.23536933958530426\n",
      "23 767 0.14883673191070557\n",
      "Validation loss: 0.26794528922833066 RMSE: 0.51763433\n",
      "24 46 0.29191234707832336\n",
      "24 96 0.43712538480758667\n",
      "24 146 0.39690718054771423\n",
      "24 196 0.2906843423843384\n",
      "24 246 0.18994490802288055\n",
      "24 296 0.24725982546806335\n",
      "24 346 0.3699303865432739\n",
      "24 396 0.24829798936843872\n",
      "24 446 0.19063112139701843\n",
      "24 496 0.17032697796821594\n",
      "24 546 0.3068360686302185\n",
      "24 596 0.12510956823825836\n",
      "24 646 0.18832005560398102\n",
      "24 696 0.3987431526184082\n",
      "24 746 0.2720101475715637\n",
      "Validation loss: 0.254638261435663 RMSE: 0.504617\n",
      "25 25 0.36541450023651123\n",
      "25 75 0.23157154023647308\n",
      "25 125 0.10320024192333221\n",
      "25 175 0.25099384784698486\n",
      "25 225 0.31471940875053406\n",
      "25 275 0.18012991547584534\n",
      "25 325 0.3261297643184662\n",
      "25 375 0.20692962408065796\n",
      "25 425 0.3998652696609497\n",
      "25 475 0.292883038520813\n",
      "25 525 0.18538710474967957\n",
      "25 575 0.21544745564460754\n",
      "25 625 0.30884766578674316\n",
      "25 675 0.32134726643562317\n",
      "25 725 0.20421895384788513\n",
      "Validation loss: 0.25320428560542707 RMSE: 0.5031941\n",
      "26 4 0.20044025778770447\n",
      "26 54 0.31668591499328613\n",
      "26 104 0.09814374148845673\n",
      "26 154 0.16054584085941315\n",
      "26 204 0.24487486481666565\n",
      "26 254 0.2156876027584076\n",
      "26 304 0.44102221727371216\n",
      "26 354 0.4006749391555786\n",
      "26 404 0.11724171042442322\n",
      "26 454 0.5229204893112183\n",
      "26 504 0.17206883430480957\n",
      "26 554 0.27792471647262573\n",
      "26 604 0.25505518913269043\n",
      "26 654 0.3175795078277588\n",
      "26 704 0.3483232259750366\n",
      "26 754 0.22123640775680542\n",
      "Validation loss: 0.26575340692173033 RMSE: 0.51551276\n",
      "27 33 0.30103689432144165\n",
      "27 83 0.3807782530784607\n",
      "27 133 0.5039612054824829\n",
      "27 183 0.20996642112731934\n",
      "27 233 0.14306055009365082\n",
      "27 283 0.25195246934890747\n",
      "27 333 0.34796613454818726\n",
      "27 383 0.6107519268989563\n",
      "27 433 0.22729435563087463\n",
      "27 483 0.17489296197891235\n",
      "27 533 0.19588419795036316\n",
      "27 583 0.36007314920425415\n",
      "27 633 0.41320061683654785\n",
      "27 683 0.12490583956241608\n",
      "27 733 0.3592723608016968\n",
      "Validation loss: 0.25312397064750686 RMSE: 0.5031143\n",
      "28 12 0.1576228141784668\n",
      "28 62 0.11688469350337982\n",
      "28 112 0.6944756507873535\n",
      "28 162 0.2296561598777771\n",
      "28 212 0.14919668436050415\n",
      "28 262 0.18885651230812073\n",
      "28 312 0.23685301840305328\n",
      "28 362 0.55724036693573\n",
      "28 412 0.20235857367515564\n",
      "28 462 0.2257494330406189\n",
      "28 512 0.3773811161518097\n",
      "28 562 0.3985952138900757\n",
      "28 612 0.33060961961746216\n",
      "28 662 0.1858370006084442\n",
      "28 712 0.40477582812309265\n",
      "28 762 0.3375401496887207\n",
      "Validation loss: 0.2451164513168966 RMSE: 0.49509236\n",
      "29 41 0.14568912982940674\n",
      "29 91 0.24211058020591736\n",
      "29 141 0.11723263561725616\n",
      "29 191 0.2572331726551056\n",
      "29 241 0.2565419673919678\n",
      "29 291 0.22585777938365936\n",
      "29 341 0.4815344214439392\n",
      "29 391 0.2461298406124115\n",
      "29 441 0.28953948616981506\n",
      "29 491 0.22580401599407196\n",
      "29 541 0.3022388517856598\n",
      "29 591 0.10358033329248428\n",
      "29 641 0.24804921448230743\n",
      "29 691 0.28577184677124023\n",
      "29 741 0.1027117520570755\n",
      "Validation loss: 0.24544012141140067 RMSE: 0.49541914\n",
      "Loaded trained model with success.\n",
      "Test loss: 0.29969772920813614 Test RMSE: 0.54744655\n",
      "Running on: cuda:0\n",
      "Loaded pre-trained model with success.\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 11.083541870117188\n",
      "0 50 0.3942428231239319\n",
      "0 100 0.42627376317977905\n",
      "0 150 0.36244094371795654\n",
      "0 200 0.5345454812049866\n",
      "0 250 0.5299948453903198\n",
      "0 300 0.3649051785469055\n",
      "0 350 1.0392298698425293\n",
      "0 400 0.6134968400001526\n",
      "0 450 0.6220345497131348\n",
      "0 500 0.4389854669570923\n",
      "0 550 0.230534166097641\n",
      "0 600 0.9121418595314026\n",
      "0 650 0.1884051263332367\n",
      "0 700 0.2721221446990967\n",
      "0 750 0.2399614155292511\n",
      "Validation loss: 0.4909842699108755 RMSE: 0.7007027\n",
      "1 29 0.4350835084915161\n",
      "1 79 0.7651926279067993\n",
      "1 129 0.31189465522766113\n",
      "1 179 0.3243834972381592\n",
      "1 229 0.36234143376350403\n",
      "1 279 0.4503355622291565\n",
      "1 329 0.4926971197128296\n",
      "1 379 0.24276775121688843\n",
      "1 429 0.3955773711204529\n",
      "1 479 0.3832363188266754\n",
      "1 529 0.2578219771385193\n",
      "1 579 0.2996722459793091\n",
      "1 629 0.32600337266921997\n",
      "1 679 0.3315262794494629\n",
      "1 729 0.18709619343280792\n",
      "Validation loss: 0.3965743804240928 RMSE: 0.6297415\n",
      "2 8 0.47815051674842834\n",
      "2 58 0.2732055187225342\n",
      "2 108 0.26783302426338196\n",
      "2 158 0.7582325339317322\n",
      "2 208 0.24775740504264832\n",
      "2 258 0.41484883427619934\n",
      "2 308 0.3979344367980957\n",
      "2 358 0.5471874475479126\n",
      "2 408 0.4689280390739441\n",
      "2 458 0.23733478784561157\n",
      "2 508 0.8571950793266296\n",
      "2 558 0.3490865230560303\n",
      "2 608 0.4694760739803314\n",
      "2 658 0.3898910880088806\n",
      "2 708 0.24359767138957977\n",
      "2 758 0.2652509808540344\n",
      "Validation loss: 0.4327133216840379 RMSE: 0.6578095\n",
      "3 37 0.41966116428375244\n",
      "3 87 0.34963321685791016\n",
      "3 137 0.3413400650024414\n",
      "3 187 0.4255739450454712\n",
      "3 237 0.3201116919517517\n",
      "3 287 0.2939832806587219\n",
      "3 337 0.2159101963043213\n",
      "3 387 0.2431820034980774\n",
      "3 437 0.4826201796531677\n",
      "3 487 0.2820107638835907\n",
      "3 537 0.2816193699836731\n",
      "3 587 0.13449512422084808\n",
      "3 637 0.3482041358947754\n",
      "3 687 0.2762139141559601\n",
      "3 737 0.979600191116333\n",
      "Validation loss: 0.42696138208403306 RMSE: 0.65342283\n",
      "4 16 0.25832390785217285\n",
      "4 66 0.6055504679679871\n",
      "4 116 0.21074643731117249\n",
      "4 166 0.2265966832637787\n",
      "4 216 0.3282815217971802\n",
      "4 266 0.18452239036560059\n",
      "4 316 0.5394518375396729\n",
      "4 366 0.4467889070510864\n",
      "4 416 0.3187653422355652\n",
      "4 466 0.23902499675750732\n",
      "4 516 0.2735286056995392\n",
      "4 566 0.29795053601264954\n",
      "4 616 0.2624552845954895\n",
      "4 666 0.27902787923812866\n",
      "4 716 0.38221800327301025\n",
      "4 766 0.5746157169342041\n",
      "Validation loss: 0.36326298468253193 RMSE: 0.602713\n",
      "5 45 0.535592794418335\n",
      "5 95 0.32954180240631104\n",
      "5 145 0.39416244626045227\n",
      "5 195 0.2087605893611908\n",
      "5 245 0.48887914419174194\n",
      "5 295 0.46051961183547974\n",
      "5 345 0.24566909670829773\n",
      "5 395 0.21520423889160156\n",
      "5 445 0.36031633615493774\n",
      "5 495 0.38044506311416626\n",
      "5 545 0.3055277466773987\n",
      "5 595 0.453441858291626\n",
      "5 645 0.21255622804164886\n",
      "5 695 0.41259434819221497\n",
      "5 745 0.731413722038269\n",
      "Validation loss: 0.37497805825927677 RMSE: 0.6123545\n",
      "6 24 0.2650330364704132\n",
      "6 74 0.30555233359336853\n",
      "6 124 0.8934219479560852\n",
      "6 174 0.17520514130592346\n",
      "6 224 0.243035227060318\n",
      "6 274 0.22982239723205566\n",
      "6 324 0.8406906127929688\n",
      "6 374 0.27547043561935425\n",
      "6 424 0.6082700490951538\n",
      "6 474 0.42327210307121277\n",
      "6 524 0.47220879793167114\n",
      "6 574 0.3608590364456177\n",
      "6 624 0.3858506679534912\n",
      "6 674 0.43875986337661743\n",
      "6 724 0.2374245524406433\n",
      "Validation loss: 0.4219153001356651 RMSE: 0.6495501\n",
      "7 3 1.128584861755371\n",
      "7 53 0.14902812242507935\n",
      "7 103 0.3909761905670166\n",
      "7 153 0.1528632938861847\n",
      "7 203 0.34353911876678467\n",
      "7 253 0.34110361337661743\n",
      "7 303 0.2999069392681122\n",
      "7 353 0.258247971534729\n",
      "7 403 0.18615993857383728\n",
      "7 453 0.5544266700744629\n",
      "7 503 0.28591102361679077\n",
      "7 553 0.23125803470611572\n",
      "7 603 0.15174831449985504\n",
      "7 653 0.21470707654953003\n",
      "7 703 0.19311299920082092\n",
      "7 753 0.49191102385520935\n",
      "Validation loss: 0.3474692976759637 RMSE: 0.58946526\n",
      "8 32 0.2992374300956726\n",
      "8 82 0.6947539448738098\n",
      "8 132 0.4913542568683624\n",
      "8 182 0.321808785200119\n",
      "8 232 0.2828759551048279\n",
      "8 282 0.22620806097984314\n",
      "8 332 0.2808322608470917\n",
      "8 382 0.24480009078979492\n",
      "8 432 0.2301921248435974\n",
      "8 482 0.2537935674190521\n",
      "8 532 0.24534113705158234\n",
      "8 582 0.265290766954422\n",
      "8 632 0.7698624134063721\n",
      "8 682 0.6290980577468872\n",
      "8 732 0.4396519064903259\n",
      "Validation loss: 0.36581718417651515 RMSE: 0.60482824\n",
      "9 11 0.7238803505897522\n",
      "9 61 0.4236092269420624\n",
      "9 111 0.24478186666965485\n",
      "9 161 0.3092876672744751\n",
      "9 211 0.5086408257484436\n",
      "9 261 0.30576804280281067\n",
      "9 311 0.3549566864967346\n",
      "9 361 0.16132280230522156\n",
      "9 411 0.179986834526062\n",
      "9 461 0.4762006402015686\n",
      "9 511 0.5564532279968262\n",
      "9 561 0.3223640024662018\n",
      "9 611 0.2588992416858673\n",
      "9 661 0.22000524401664734\n",
      "9 711 0.292796790599823\n",
      "9 761 0.731662392616272\n",
      "Validation loss: 0.3516317671712707 RMSE: 0.5929855\n",
      "10 40 0.3842926621437073\n",
      "10 90 0.21367734670639038\n",
      "10 140 0.5741714239120483\n",
      "10 190 0.3652796447277069\n",
      "10 240 0.7247167825698853\n",
      "10 290 0.21410715579986572\n",
      "10 340 0.4888421297073364\n",
      "10 390 0.49349284172058105\n",
      "10 440 0.36628803610801697\n",
      "10 490 0.11569138616323471\n",
      "10 540 0.20442412793636322\n",
      "10 590 0.1874905824661255\n",
      "10 640 0.1946684569120407\n",
      "10 690 0.23285822570323944\n",
      "10 740 0.2422417551279068\n",
      "Validation loss: 0.35127575728384886 RMSE: 0.5926852\n",
      "11 19 0.3486959636211395\n",
      "11 69 0.3804463744163513\n",
      "11 119 0.39239779114723206\n",
      "11 169 0.30411025881767273\n",
      "11 219 0.3200358748435974\n",
      "11 269 0.5085513591766357\n",
      "11 319 0.18053480982780457\n",
      "11 369 0.14024794101715088\n",
      "11 419 0.21579694747924805\n",
      "11 469 0.3614649772644043\n",
      "11 519 0.12615308165550232\n",
      "11 569 0.30165791511535645\n",
      "11 619 0.233439639210701\n",
      "11 669 0.13075067102909088\n",
      "11 719 0.32395991683006287\n",
      "11 769 0.2446502149105072\n",
      "Validation loss: 0.365535362952334 RMSE: 0.60459524\n",
      "12 48 0.44232994318008423\n",
      "12 98 0.5100067257881165\n",
      "12 148 0.26400524377822876\n",
      "12 198 0.19104619324207306\n",
      "12 248 0.2611970901489258\n",
      "12 298 0.4368334710597992\n",
      "12 348 0.24432308971881866\n",
      "12 398 0.24599206447601318\n",
      "12 448 0.2919786274433136\n",
      "12 498 0.3435244858264923\n",
      "12 548 0.185179203748703\n",
      "12 598 0.519568681716919\n",
      "12 648 0.17414402961730957\n",
      "12 698 0.15211258828639984\n",
      "12 748 0.1744837909936905\n",
      "Validation loss: 0.3361425950904103 RMSE: 0.5797781\n",
      "13 27 0.18039026856422424\n",
      "13 77 0.2168121039867401\n",
      "13 127 0.15004652738571167\n",
      "13 177 0.3069872260093689\n",
      "13 227 0.7072296142578125\n",
      "13 277 0.19657978415489197\n",
      "13 327 0.14823287725448608\n",
      "13 377 0.20186898112297058\n",
      "13 427 0.29559019207954407\n",
      "13 477 0.7466616630554199\n",
      "13 527 0.22897306084632874\n",
      "13 577 0.4814613461494446\n",
      "13 627 0.26479488611221313\n",
      "13 677 0.2884769141674042\n",
      "13 727 0.2803308069705963\n",
      "Validation loss: 0.35737103015622673 RMSE: 0.5978052\n",
      "14 6 0.252865195274353\n",
      "14 56 0.27758026123046875\n",
      "14 106 0.42500942945480347\n",
      "14 156 0.16994431614875793\n",
      "14 206 0.30781781673431396\n",
      "14 256 0.23421619832515717\n",
      "14 306 0.18600094318389893\n",
      "14 356 0.17969663441181183\n",
      "14 406 0.2813533544540405\n",
      "14 456 0.30603736639022827\n",
      "14 506 0.23721958696842194\n",
      "14 556 0.2715761661529541\n",
      "14 606 0.22996178269386292\n",
      "14 656 0.1626095473766327\n",
      "14 706 0.835720419883728\n",
      "14 756 0.37877917289733887\n",
      "Validation loss: 0.32603653925744924 RMSE: 0.5709961\n",
      "15 35 0.3049459457397461\n",
      "15 85 0.12878037989139557\n",
      "15 135 0.14836198091506958\n",
      "15 185 0.15616360306739807\n",
      "15 235 0.3140067756175995\n",
      "15 285 0.4009092450141907\n",
      "15 335 0.10992492735385895\n",
      "15 385 0.2454499453306198\n",
      "15 435 0.21017102897167206\n",
      "15 485 0.32046645879745483\n",
      "15 535 0.2779802680015564\n",
      "15 585 0.8940821290016174\n",
      "15 635 0.7426977753639221\n",
      "15 685 0.2020895779132843\n",
      "15 735 0.179983988404274\n",
      "Validation loss: 0.33012441868948583 RMSE: 0.5745645\n",
      "16 14 0.11904994398355484\n",
      "16 64 0.3904692530632019\n",
      "16 114 0.0996408611536026\n",
      "16 164 0.2701931595802307\n",
      "16 214 0.6867249011993408\n",
      "16 264 0.5140464901924133\n",
      "16 314 0.14333930611610413\n",
      "16 364 0.17409996688365936\n",
      "16 414 0.21994967758655548\n",
      "16 464 0.1729058474302292\n",
      "16 514 0.27025753259658813\n",
      "16 564 0.31652241945266724\n",
      "16 614 0.49129927158355713\n",
      "16 664 0.35840877890586853\n",
      "16 714 0.3124251067638397\n",
      "16 764 0.20984786748886108\n",
      "Validation loss: 0.35233581805711284 RMSE: 0.5935788\n",
      "17 43 0.22172322869300842\n",
      "17 93 0.19768479466438293\n",
      "17 143 0.20164825022220612\n",
      "17 193 0.3082423210144043\n",
      "17 243 0.26528051495552063\n",
      "17 293 0.8497776985168457\n",
      "17 343 0.2816666066646576\n",
      "17 393 0.24938860535621643\n",
      "17 443 0.212061807513237\n",
      "17 493 0.42988741397857666\n",
      "17 543 0.2752951681613922\n",
      "17 593 0.2635662257671356\n",
      "17 643 0.19614291191101074\n",
      "17 693 0.2020154893398285\n",
      "17 743 0.35088929533958435\n",
      "Validation loss: 0.35149147635435357 RMSE: 0.5928672\n",
      "18 22 0.42323434352874756\n",
      "18 72 0.27003222703933716\n",
      "18 122 0.1730116903781891\n",
      "18 172 0.18570448458194733\n",
      "18 222 0.34538087248802185\n",
      "18 272 0.39771169424057007\n",
      "18 322 0.18110445141792297\n",
      "18 372 0.23552441596984863\n",
      "18 422 0.24192985892295837\n",
      "18 472 0.2179275006055832\n",
      "18 522 0.10920384526252747\n",
      "18 572 0.4549233317375183\n",
      "18 622 0.4817492365837097\n",
      "18 672 0.20683497190475464\n",
      "18 722 0.15524685382843018\n",
      "Validation loss: 0.32701745262259946 RMSE: 0.5718544\n",
      "19 1 0.22464683651924133\n",
      "19 51 0.17633089423179626\n",
      "19 101 0.26868700981140137\n",
      "19 151 0.39088186621665955\n",
      "19 201 0.2803907096385956\n",
      "19 251 0.305460661649704\n",
      "19 301 0.22140806913375854\n",
      "19 351 0.37350428104400635\n",
      "19 401 0.10125745832920074\n",
      "19 451 0.19165915250778198\n",
      "19 501 0.36181777715682983\n",
      "19 551 0.2691686153411865\n",
      "19 601 0.27272677421569824\n",
      "19 651 0.22154976427555084\n",
      "19 701 0.14231695234775543\n",
      "19 751 0.3183765411376953\n",
      "Validation loss: 0.35784607813419667 RMSE: 0.59820235\n",
      "20 30 0.2544475495815277\n",
      "20 80 0.20063650608062744\n",
      "20 130 0.4806399345397949\n",
      "20 180 0.3859362304210663\n",
      "20 230 0.17996084690093994\n",
      "20 280 0.44924527406692505\n",
      "20 330 0.2090059518814087\n",
      "20 380 0.30048033595085144\n",
      "20 430 0.1888033002614975\n",
      "20 480 0.40430039167404175\n",
      "20 530 0.30128589272499084\n",
      "20 580 0.38479888439178467\n",
      "20 630 0.2643730044364929\n",
      "20 680 0.17017033696174622\n",
      "20 730 0.30587273836135864\n",
      "Validation loss: 0.3433842229492524 RMSE: 0.58598995\n",
      "21 9 0.2726372480392456\n",
      "21 59 0.22788485884666443\n",
      "21 109 0.7834730744361877\n",
      "21 159 0.30029934644699097\n",
      "21 209 0.13696414232254028\n",
      "21 259 0.33862531185150146\n",
      "21 309 0.3509209156036377\n",
      "21 359 0.27806711196899414\n",
      "21 409 0.3291674852371216\n",
      "21 459 0.33303505182266235\n",
      "21 509 0.23188552260398865\n",
      "21 559 0.17728443443775177\n",
      "21 609 0.4093439280986786\n",
      "21 659 0.3511190116405487\n",
      "21 709 0.20043480396270752\n",
      "21 759 0.3410140573978424\n",
      "Validation loss: 0.3563883414711146 RMSE: 0.5969827\n",
      "22 38 0.3105051517486572\n",
      "22 88 0.18811264634132385\n",
      "22 138 0.29615518450737\n",
      "22 188 0.12274466454982758\n",
      "22 238 0.29077038168907166\n",
      "22 288 0.288339227437973\n",
      "22 338 0.39970576763153076\n",
      "22 388 0.20686963200569153\n",
      "22 438 0.17966175079345703\n",
      "22 488 0.18230421841144562\n",
      "22 538 0.2215464860200882\n",
      "22 588 0.25413814187049866\n",
      "22 638 0.1843547224998474\n",
      "22 688 0.19567734003067017\n",
      "22 738 0.17407304048538208\n",
      "Validation loss: 0.3264483632519841 RMSE: 0.5713566\n",
      "23 17 0.3721621632575989\n",
      "23 67 0.17559659481048584\n",
      "23 117 0.27205848693847656\n",
      "23 167 0.3615300953388214\n",
      "23 217 0.23282061517238617\n",
      "23 267 0.21009638905525208\n",
      "23 317 0.14900948107242584\n",
      "23 367 0.17844539880752563\n",
      "23 417 0.16881069540977478\n",
      "23 467 0.16188420355319977\n",
      "23 517 0.11599067598581314\n",
      "23 567 0.11567720025777817\n",
      "23 617 0.2703793942928314\n",
      "23 667 0.36511871218681335\n",
      "23 717 0.5049893260002136\n",
      "23 767 0.2968845069408417\n",
      "Validation loss: 0.33623621771659923 RMSE: 0.5798588\n",
      "24 46 0.307300329208374\n",
      "24 96 0.13157929480075836\n",
      "24 146 0.2489909678697586\n",
      "24 196 0.24454852938652039\n",
      "24 246 0.5320668816566467\n",
      "24 296 0.3181620240211487\n",
      "24 346 0.4821678102016449\n",
      "24 396 0.22964555025100708\n",
      "24 446 0.21411804854869843\n",
      "24 496 0.3075716495513916\n",
      "24 546 0.3146290183067322\n",
      "24 596 0.21873998641967773\n",
      "24 646 0.23210948705673218\n",
      "24 696 0.13114193081855774\n",
      "24 746 0.3754843771457672\n",
      "Validation loss: 0.3348396402400206 RMSE: 0.5786533\n",
      "25 25 0.1912168562412262\n",
      "25 75 0.2166239470243454\n",
      "25 125 0.1430436223745346\n",
      "25 175 0.17365938425064087\n",
      "25 225 0.5808515548706055\n",
      "25 275 0.34249669313430786\n",
      "25 325 0.19058719277381897\n",
      "25 375 0.33286964893341064\n",
      "25 425 0.3438958525657654\n",
      "25 475 0.2790347933769226\n",
      "25 525 0.1664443016052246\n",
      "25 575 0.1913978010416031\n",
      "25 625 0.15418922901153564\n",
      "25 675 0.3249906301498413\n",
      "25 725 0.18184515833854675\n",
      "Validation loss: 0.31725424862302404 RMSE: 0.5632533\n",
      "26 4 0.2150883823633194\n",
      "26 54 0.23006437718868256\n",
      "26 104 0.25680896639823914\n",
      "26 154 0.09781816601753235\n",
      "26 204 0.3903716206550598\n",
      "26 254 0.17509683966636658\n",
      "26 304 0.18774573504924774\n",
      "26 354 0.3566510081291199\n",
      "26 404 0.34256869554519653\n",
      "26 454 0.1739445924758911\n",
      "26 504 0.6046448945999146\n",
      "26 554 0.21823781728744507\n",
      "26 604 0.28694969415664673\n",
      "26 654 0.1829831749200821\n",
      "26 704 0.24778211116790771\n",
      "26 754 0.16391324996948242\n",
      "Validation loss: 0.32322470421957616 RMSE: 0.56852853\n",
      "27 33 0.12931880354881287\n",
      "27 83 0.2425168752670288\n",
      "27 133 0.4499353766441345\n",
      "27 183 0.3359830677509308\n",
      "27 233 0.15061704814434052\n",
      "27 283 0.25145232677459717\n",
      "27 333 0.5931284427642822\n",
      "27 383 0.26237019896507263\n",
      "27 433 0.27190694212913513\n",
      "27 483 0.19403307139873505\n",
      "27 533 0.15851368010044098\n",
      "27 583 0.10100466758012772\n",
      "27 633 0.4493878483772278\n",
      "27 683 0.07562486827373505\n",
      "27 733 0.16534355282783508\n",
      "Validation loss: 0.30373246891095357 RMSE: 0.5511193\n",
      "28 12 0.17356275022029877\n",
      "28 62 0.18647944927215576\n",
      "28 112 0.3484119772911072\n",
      "28 162 0.18894965946674347\n",
      "28 212 0.24087031185626984\n",
      "28 262 0.19476526975631714\n",
      "28 312 0.2662450969219208\n",
      "28 362 0.31235843896865845\n",
      "28 412 0.19830305874347687\n",
      "28 462 0.2620300054550171\n",
      "28 512 0.20169290900230408\n",
      "28 562 0.24706867337226868\n",
      "28 612 0.13966426253318787\n",
      "28 662 0.16225439310073853\n",
      "28 712 0.16627569496631622\n",
      "28 762 0.1941860318183899\n",
      "Validation loss: 0.3085514460964238 RMSE: 0.5554741\n",
      "29 41 0.10565540194511414\n",
      "29 91 0.11397216469049454\n",
      "29 141 0.20495887100696564\n",
      "29 191 0.2820020616054535\n",
      "29 241 0.2554357647895813\n",
      "29 291 0.8285275101661682\n",
      "29 341 0.1347576230764389\n",
      "29 391 0.4076633155345917\n",
      "29 441 0.19139569997787476\n",
      "29 491 0.152267187833786\n",
      "29 541 0.16459983587265015\n",
      "29 591 0.29016387462615967\n",
      "29 641 0.19990681111812592\n",
      "29 691 0.1678549349308014\n",
      "29 741 0.22519363462924957\n",
      "Validation loss: 0.3154051071032882 RMSE: 0.5616094\n",
      "Loaded trained model with success.\n",
      "Test loss: 0.29551472904960213 Test RMSE: 0.54361266\n",
      "Running on: cuda:0\n",
      "Loaded pre-trained model with success.\n",
      "pred_head.0.weight True\n",
      "pred_head.0.bias True\n",
      "pred_head.2.weight True\n",
      "pred_head.2.bias True\n",
      "pred_head.4.weight True\n",
      "pred_head.4.bias True\n",
      "0 0 10.931605339050293\n",
      "0 50 0.46799078583717346\n",
      "0 100 0.3379665017127991\n",
      "0 150 0.1288267970085144\n",
      "0 200 0.3864282965660095\n",
      "0 250 0.18072307109832764\n",
      "0 300 0.2724389433860779\n",
      "0 350 0.37379390001296997\n",
      "0 400 0.6158547401428223\n",
      "0 450 0.3038257360458374\n",
      "0 500 0.3346134126186371\n",
      "0 550 0.16920460760593414\n",
      "0 600 0.28240182995796204\n",
      "0 650 0.33482056856155396\n",
      "0 700 0.4226078391075134\n",
      "0 750 0.7018125653266907\n",
      "Validation loss: 0.45059648561565313 RMSE: 0.6712649\n",
      "1 29 0.7566782236099243\n",
      "1 79 0.24832558631896973\n",
      "1 129 0.4736529588699341\n",
      "1 179 0.6992709636688232\n",
      "1 229 0.306742399930954\n",
      "1 279 0.39840561151504517\n",
      "1 329 0.6540940403938293\n",
      "1 379 0.2563199996948242\n",
      "1 429 0.4237406551837921\n",
      "1 479 0.7593087553977966\n",
      "1 529 0.2879575788974762\n",
      "1 579 0.2944818139076233\n",
      "1 629 0.22426900267601013\n",
      "1 679 0.29608479142189026\n",
      "1 729 0.344831645488739\n",
      "Validation loss: 0.3971879281761015 RMSE: 0.63022846\n",
      "2 8 0.45735272765159607\n",
      "2 58 0.24504944682121277\n",
      "2 108 0.9721766710281372\n",
      "2 158 0.4230707287788391\n",
      "2 208 0.7065932154655457\n",
      "2 258 0.7164842486381531\n",
      "2 308 0.5230011940002441\n",
      "2 358 0.23244620859622955\n",
      "2 408 0.4844098687171936\n",
      "2 458 0.25240886211395264\n",
      "2 508 0.23449984192848206\n",
      "2 558 0.3392926752567291\n",
      "2 608 0.329621821641922\n",
      "2 658 0.259854257106781\n",
      "2 708 0.22529999911785126\n",
      "2 758 0.7103749513626099\n",
      "Validation loss: 0.42842942158527236 RMSE: 0.6545452\n",
      "3 37 0.38387277722358704\n",
      "3 87 0.34846770763397217\n",
      "3 137 0.780998945236206\n",
      "3 187 0.3057922124862671\n",
      "3 237 0.11149807274341583\n",
      "3 287 0.6069695949554443\n",
      "3 337 0.32717132568359375\n",
      "3 387 0.19861549139022827\n",
      "3 437 0.3209715485572815\n",
      "3 487 0.30795273184776306\n",
      "3 537 0.22738391160964966\n",
      "3 587 0.2320316880941391\n",
      "3 637 0.28545045852661133\n",
      "3 687 0.4337904751300812\n",
      "3 737 0.47850361466407776\n",
      "Validation loss: 0.3941490803571308 RMSE: 0.6278129\n",
      "4 16 0.18068866431713104\n",
      "4 66 0.41640642285346985\n",
      "4 116 0.3227750062942505\n",
      "4 166 0.16248980164527893\n",
      "4 216 0.448723167181015\n",
      "4 266 0.3558098077774048\n",
      "4 316 0.36064624786376953\n",
      "4 366 0.33321911096572876\n",
      "4 416 0.2744418680667877\n",
      "4 466 0.28131040930747986\n",
      "4 516 0.8161174058914185\n",
      "4 566 0.24891698360443115\n",
      "4 616 0.339194655418396\n",
      "4 666 0.27556589245796204\n",
      "4 716 0.2505127787590027\n",
      "4 766 0.2899734079837799\n",
      "Validation loss: 0.5191883196068161 RMSE: 0.7205472\n",
      "5 45 0.20565161108970642\n",
      "5 95 0.2267168015241623\n",
      "5 145 0.2390017807483673\n",
      "5 195 0.20058023929595947\n",
      "5 245 0.11855839192867279\n",
      "5 295 0.7051184177398682\n",
      "5 345 0.19192031025886536\n",
      "5 395 0.3328198492527008\n",
      "5 445 0.35545849800109863\n",
      "5 495 0.32599273324012756\n",
      "5 545 0.13176561892032623\n",
      "5 595 0.177333801984787\n",
      "5 645 0.3551127314567566\n",
      "5 695 0.46949446201324463\n",
      "5 745 0.5893179178237915\n",
      "Validation loss: 0.42721615622148795 RMSE: 0.65361774\n",
      "6 24 1.0719828605651855\n",
      "6 74 0.2846609055995941\n",
      "6 124 0.12025794386863708\n",
      "6 174 0.2622130513191223\n",
      "6 224 0.26221391558647156\n",
      "6 274 0.3637482225894928\n",
      "6 324 0.31124383211135864\n",
      "6 374 0.2652166783809662\n",
      "6 424 0.42401182651519775\n",
      "6 474 0.3206281363964081\n",
      "6 524 0.21634580194950104\n",
      "6 574 0.3024202287197113\n",
      "6 624 0.3095162808895111\n",
      "6 674 0.27515101432800293\n",
      "6 724 0.2682778239250183\n",
      "Validation loss: 0.3742761757663068 RMSE: 0.6117812\n",
      "7 3 0.2945769429206848\n",
      "7 53 0.48109346628189087\n",
      "7 103 0.33814603090286255\n",
      "7 153 0.42978113889694214\n",
      "7 203 0.10382916778326035\n",
      "7 253 0.2644016146659851\n",
      "7 303 0.257718026638031\n",
      "7 353 0.1820111721754074\n",
      "7 403 0.23673328757286072\n",
      "7 453 0.17033202946186066\n",
      "7 503 0.13377192616462708\n",
      "7 553 0.31952905654907227\n",
      "7 603 0.2910785675048828\n",
      "7 653 0.15892961621284485\n",
      "7 703 0.23760907351970673\n",
      "7 753 0.20815786719322205\n",
      "Validation loss: 0.38559978488175306 RMSE: 0.62096685\n",
      "8 32 0.22413089871406555\n",
      "8 82 0.2858625054359436\n",
      "8 132 0.2862892150878906\n",
      "8 182 0.48925188183784485\n",
      "8 232 0.20500318706035614\n",
      "8 282 0.21303044259548187\n",
      "8 332 0.30866044759750366\n",
      "8 382 0.24573272466659546\n",
      "8 432 0.41947031021118164\n",
      "8 482 0.1885465532541275\n",
      "8 532 0.16825732588768005\n",
      "8 582 0.23213469982147217\n",
      "8 632 0.12453240901231766\n",
      "8 682 0.3607270121574402\n",
      "8 732 0.2206631451845169\n",
      "Validation loss: 0.3768358270582907 RMSE: 0.61386955\n",
      "9 11 0.2263346016407013\n",
      "9 61 0.1399804651737213\n",
      "9 111 0.4130091071128845\n",
      "9 161 0.7778892517089844\n",
      "9 211 0.3271082043647766\n",
      "9 261 0.19383318722248077\n",
      "9 311 0.4619333744049072\n",
      "9 361 0.39168858528137207\n",
      "9 411 0.12136615812778473\n",
      "9 461 0.20084278285503387\n",
      "9 511 0.36771005392074585\n",
      "9 561 0.2752606272697449\n",
      "9 611 0.3073340058326721\n",
      "9 661 0.18481579422950745\n",
      "9 711 0.25096994638442993\n",
      "9 761 0.3138844668865204\n",
      "Validation loss: 0.37553392065798535 RMSE: 0.61280817\n",
      "10 40 0.9555469751358032\n",
      "10 90 0.42544612288475037\n",
      "10 140 0.18828275799751282\n",
      "10 190 0.15470503270626068\n",
      "10 240 0.3832830786705017\n",
      "10 290 0.20156538486480713\n",
      "10 340 0.5187259912490845\n",
      "10 390 0.3371822237968445\n",
      "10 440 0.31658196449279785\n",
      "10 490 0.7362124919891357\n",
      "10 540 0.1646314114332199\n",
      "10 590 0.2522193491458893\n",
      "10 640 0.5509842038154602\n",
      "10 690 0.4482012093067169\n",
      "10 740 0.24755167961120605\n",
      "Validation loss: 0.3809814737452304 RMSE: 0.617237\n",
      "11 19 0.18609315156936646\n",
      "11 69 0.1465800255537033\n",
      "11 119 0.2968142032623291\n",
      "11 169 0.22945192456245422\n",
      "11 219 0.21029044687747955\n",
      "11 269 0.23973435163497925\n",
      "11 319 0.20987510681152344\n",
      "11 369 0.32583725452423096\n",
      "11 419 0.22323158383369446\n",
      "11 469 0.8216453790664673\n",
      "11 519 0.2332107573747635\n",
      "11 569 0.38577163219451904\n",
      "11 619 0.28815656900405884\n",
      "11 669 0.6222373247146606\n",
      "11 719 0.20815938711166382\n",
      "11 769 0.3244040608406067\n",
      "Validation loss: 0.3582195784787045 RMSE: 0.5985145\n",
      "12 48 0.318440318107605\n",
      "12 98 0.4453345239162445\n",
      "12 148 0.19653254747390747\n",
      "12 198 0.22575031220912933\n",
      "12 248 0.19671793282032013\n",
      "12 298 0.2925138473510742\n",
      "12 348 0.2868138551712036\n",
      "12 398 0.2893054485321045\n",
      "12 448 0.5393132567405701\n",
      "12 498 0.28473055362701416\n",
      "12 548 0.654934287071228\n",
      "12 598 0.2208782434463501\n",
      "12 648 0.2619289755821228\n",
      "12 698 0.20145964622497559\n",
      "12 748 0.08353952318429947\n",
      "Validation loss: 0.35380243471659284 RMSE: 0.5948129\n",
      "13 27 0.3280622959136963\n",
      "13 77 0.2074125111103058\n",
      "13 127 0.12645560503005981\n",
      "13 177 0.3053024709224701\n",
      "13 227 0.27038222551345825\n",
      "13 277 0.21799491345882416\n",
      "13 327 0.55879807472229\n",
      "13 377 0.38764867186546326\n",
      "13 427 0.1407308131456375\n",
      "13 477 0.24843841791152954\n",
      "13 527 0.21167153120040894\n",
      "13 577 0.21701358258724213\n",
      "13 627 0.4034149646759033\n",
      "13 677 0.2515751123428345\n",
      "13 727 0.19435229897499084\n",
      "Validation loss: 0.3528012778500424 RMSE: 0.5939708\n",
      "14 6 0.21778762340545654\n",
      "14 56 0.17697350680828094\n",
      "14 106 0.19244028627872467\n",
      "14 156 0.24654461443424225\n",
      "14 206 0.3686835765838623\n",
      "14 256 0.4100863039493561\n",
      "14 306 0.17936310172080994\n",
      "14 356 0.13134200870990753\n",
      "14 406 0.2434784471988678\n",
      "14 456 0.17785200476646423\n",
      "14 506 0.2944255769252777\n",
      "14 556 0.17482326924800873\n",
      "14 606 0.2339194118976593\n",
      "14 656 0.2793004810810089\n",
      "14 706 0.5767475366592407\n",
      "14 756 0.198826864361763\n",
      "Validation loss: 0.33950748431551103 RMSE: 0.5826727\n",
      "15 35 0.14089013636112213\n",
      "15 85 0.22641964256763458\n",
      "15 135 0.2134077548980713\n",
      "15 185 0.3233434557914734\n",
      "15 235 0.16848526895046234\n",
      "15 285 0.29792898893356323\n",
      "15 335 0.18364089727401733\n",
      "15 385 0.1846635788679123\n",
      "15 435 0.15655460953712463\n",
      "15 485 0.2588019371032715\n",
      "15 535 0.28491270542144775\n",
      "15 585 0.1874888837337494\n",
      "15 635 0.3408524692058563\n",
      "15 685 0.1945182830095291\n",
      "15 735 0.2437760829925537\n",
      "Validation loss: 0.3624860340291086 RMSE: 0.6020681\n",
      "16 14 0.20389725267887115\n",
      "16 64 0.48580753803253174\n",
      "16 114 0.4257080852985382\n",
      "16 164 0.36389437317848206\n",
      "16 214 0.2856040596961975\n",
      "16 264 0.20462480187416077\n",
      "16 314 0.18377207219600677\n",
      "16 364 0.34830695390701294\n",
      "16 414 0.1558547019958496\n",
      "16 464 0.20845311880111694\n",
      "16 514 0.274221807718277\n",
      "16 564 0.21558721363544464\n",
      "16 614 0.6510594487190247\n",
      "16 664 0.1848297417163849\n",
      "16 714 0.17933513224124908\n",
      "16 764 0.2586623430252075\n",
      "Validation loss: 0.3383244251941933 RMSE: 0.58165663\n",
      "17 43 0.23603883385658264\n",
      "17 93 0.3880389928817749\n",
      "17 143 0.3885897994041443\n",
      "17 193 0.2100827991962433\n",
      "17 243 0.2748572528362274\n",
      "17 293 0.13849854469299316\n",
      "17 343 0.20267707109451294\n",
      "17 393 0.19951209425926208\n",
      "17 443 0.31754830479621887\n",
      "17 493 0.3450816571712494\n",
      "17 543 0.2753245532512665\n",
      "17 593 0.17009112238883972\n",
      "17 643 0.6816288828849792\n",
      "17 693 0.09830255806446075\n",
      "17 743 0.22210060060024261\n",
      "Validation loss: 0.36469775306827884 RMSE: 0.6039021\n",
      "18 22 0.3860253691673279\n",
      "18 72 0.2252482771873474\n",
      "18 122 0.3517877161502838\n",
      "18 172 0.21136415004730225\n",
      "18 222 0.30799323320388794\n",
      "18 272 0.28833115100860596\n",
      "18 322 0.266416996717453\n",
      "18 372 0.18603459000587463\n",
      "18 422 0.44020482897758484\n",
      "18 472 0.26300960779190063\n",
      "18 522 0.25390273332595825\n",
      "18 572 0.29463037848472595\n",
      "18 622 0.20208841562271118\n",
      "18 672 0.2783470153808594\n",
      "18 722 0.17387297749519348\n",
      "Validation loss: 0.3335307548918268 RMSE: 0.5775212\n",
      "19 1 0.18167319893836975\n",
      "19 51 0.1594899594783783\n",
      "19 101 0.1833171248435974\n",
      "19 151 0.1768888533115387\n",
      "19 201 0.2949918508529663\n",
      "19 251 0.30382394790649414\n",
      "19 301 0.30972445011138916\n",
      "19 351 0.3693331480026245\n",
      "19 401 0.27493464946746826\n",
      "19 451 0.21858814358711243\n",
      "19 501 0.2867927551269531\n",
      "19 551 0.13261918723583221\n",
      "19 601 0.20406410098075867\n",
      "19 651 0.2240692675113678\n",
      "19 701 0.16342057287693024\n",
      "19 751 0.1670348346233368\n",
      "Validation loss: 0.32741883862763643 RMSE: 0.57220525\n",
      "20 30 0.16892316937446594\n",
      "20 80 0.12313517183065414\n",
      "20 130 0.19865082204341888\n",
      "20 180 0.17648690938949585\n",
      "20 230 0.3424396812915802\n",
      "20 280 0.38333356380462646\n",
      "20 330 0.14054027199745178\n",
      "20 380 0.26885050535202026\n",
      "20 430 0.4940805435180664\n",
      "20 480 0.18690460920333862\n",
      "20 530 0.22226044535636902\n",
      "20 580 0.29552188515663147\n",
      "20 630 0.1292755901813507\n",
      "20 680 0.5009620189666748\n",
      "20 730 0.22823676466941833\n",
      "Validation loss: 0.31656386407420917 RMSE: 0.5626401\n",
      "21 9 0.1876930594444275\n",
      "21 59 0.14337186515331268\n",
      "21 109 0.2755706012248993\n",
      "21 159 0.22509925067424774\n",
      "21 209 0.12598180770874023\n",
      "21 259 0.2827260196208954\n",
      "21 309 0.24299967288970947\n",
      "21 359 0.21332517266273499\n",
      "21 409 0.19877110421657562\n",
      "21 459 0.3212452530860901\n",
      "21 509 0.2119421511888504\n",
      "21 559 0.17804355919361115\n",
      "21 609 0.07907608896493912\n",
      "21 659 0.19529318809509277\n",
      "21 709 0.1278332769870758\n",
      "21 759 0.2804189920425415\n",
      "Validation loss: 0.3395413164158954 RMSE: 0.58270174\n",
      "22 38 0.1873742938041687\n",
      "22 88 0.24222150444984436\n",
      "22 138 0.23784102499485016\n",
      "22 188 0.2505824863910675\n",
      "22 238 0.19139304757118225\n",
      "22 288 0.487814724445343\n",
      "22 338 0.1020534560084343\n",
      "22 388 0.2254185527563095\n",
      "22 438 0.17012815177440643\n",
      "22 488 0.28174465894699097\n",
      "22 538 0.14882850646972656\n",
      "22 588 0.3242977261543274\n",
      "22 638 0.1044875830411911\n",
      "22 688 0.23746678233146667\n",
      "22 738 0.330640971660614\n",
      "Validation loss: 0.33337744506185546 RMSE: 0.57738847\n",
      "23 17 0.09452338516712189\n",
      "23 67 0.14024829864501953\n",
      "23 117 0.17330166697502136\n",
      "23 167 0.5866367220878601\n",
      "23 217 0.11781136691570282\n",
      "23 267 0.19687938690185547\n",
      "23 317 0.22466941177845\n",
      "23 367 0.5276999473571777\n",
      "23 417 0.19953912496566772\n",
      "23 467 0.12853653728961945\n",
      "23 517 0.21876148879528046\n",
      "23 567 0.15767300128936768\n",
      "23 617 0.17862951755523682\n",
      "23 667 0.20451325178146362\n",
      "23 717 0.18111783266067505\n",
      "23 767 0.3040563762187958\n",
      "Validation loss: 0.31963053614120274 RMSE: 0.56535876\n",
      "24 46 0.37620359659194946\n",
      "24 96 0.2857441008090973\n",
      "24 146 0.2753186821937561\n",
      "24 196 0.2429867684841156\n",
      "24 246 0.3596826195716858\n",
      "24 296 0.19961519539356232\n",
      "24 346 0.20363858342170715\n",
      "24 396 0.30066147446632385\n",
      "24 446 0.1704566478729248\n",
      "24 496 0.43970823287963867\n",
      "24 546 0.16842053830623627\n",
      "24 596 0.2146088182926178\n",
      "24 646 0.1547819972038269\n",
      "24 696 0.18208779394626617\n",
      "24 746 0.1811566799879074\n",
      "Validation loss: 0.31274571050615874 RMSE: 0.5592367\n",
      "25 25 0.3233935832977295\n",
      "25 75 0.6424819827079773\n",
      "25 125 0.16692858934402466\n",
      "25 175 0.4629436731338501\n",
      "25 225 0.17945992946624756\n",
      "25 275 0.14096060395240784\n",
      "25 325 0.07605375349521637\n",
      "25 375 0.12483824044466019\n",
      "25 425 0.12154848873615265\n",
      "25 475 0.15441885590553284\n",
      "25 525 0.18638408184051514\n",
      "25 575 0.2901570200920105\n",
      "25 625 0.24859702587127686\n",
      "25 675 0.27985474467277527\n",
      "25 725 0.14647267758846283\n",
      "Validation loss: 0.3181419968057205 RMSE: 0.5640408\n",
      "26 4 0.2823314666748047\n",
      "26 54 0.21185171604156494\n",
      "26 104 0.09145559370517731\n",
      "26 154 0.20139168202877045\n",
      "26 204 0.18935570120811462\n",
      "26 254 0.15243834257125854\n",
      "26 304 0.9985036849975586\n",
      "26 354 0.14462655782699585\n",
      "26 404 0.3104846477508545\n",
      "26 454 0.20793285965919495\n",
      "26 504 0.13581891357898712\n",
      "26 554 0.3465586304664612\n",
      "26 604 0.266253799200058\n",
      "26 654 0.186090350151062\n",
      "26 704 0.12155573815107346\n",
      "26 754 0.1556198000907898\n",
      "Validation loss: 0.320329079459257 RMSE: 0.56597626\n",
      "27 33 0.377433717250824\n",
      "27 83 0.17277562618255615\n",
      "27 133 0.13467779755592346\n",
      "27 183 0.2554935812950134\n",
      "27 233 0.44941413402557373\n",
      "27 283 0.1875016689300537\n",
      "27 333 0.33762842416763306\n",
      "27 383 0.23988157510757446\n",
      "27 433 0.259517103433609\n",
      "27 483 0.11811384558677673\n",
      "27 533 0.5537766218185425\n",
      "27 583 0.2914666533470154\n",
      "27 633 0.07922202348709106\n",
      "27 683 0.1752682328224182\n",
      "27 733 0.29361364245414734\n",
      "Validation loss: 0.3196262259803274 RMSE: 0.56535494\n",
      "28 12 0.2020699828863144\n",
      "28 62 0.21058711409568787\n",
      "28 112 0.3351549506187439\n",
      "28 162 0.1615878790616989\n",
      "28 212 0.23741790652275085\n",
      "28 262 0.18581447005271912\n",
      "28 312 0.1227494403719902\n",
      "28 362 0.3816136121749878\n",
      "28 412 0.48625659942626953\n",
      "28 462 0.1106974333524704\n",
      "28 512 0.19034215807914734\n",
      "28 562 0.21857541799545288\n",
      "28 612 0.3057343661785126\n",
      "28 662 0.13035666942596436\n",
      "28 712 0.1731211096048355\n",
      "28 762 0.20192718505859375\n",
      "Validation loss: 0.3190894737182295 RMSE: 0.5648801\n",
      "29 41 0.1802116334438324\n",
      "29 91 0.16405969858169556\n",
      "29 141 0.185677170753479\n",
      "29 191 0.17788109183311462\n",
      "29 241 0.3680582642555237\n",
      "29 291 0.2659580111503601\n",
      "29 341 0.39067506790161133\n",
      "29 391 0.20393367111682892\n",
      "29 441 0.12955842912197113\n",
      "29 491 0.29681718349456787\n",
      "29 541 0.3886469900608063\n",
      "29 591 0.13519513607025146\n",
      "29 641 0.32542458176612854\n",
      "29 691 0.18850378692150116\n",
      "29 741 0.4580933451652527\n",
      "Validation loss: 0.30879724096945105 RMSE: 0.5556953\n",
      "Loaded trained model with success.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-10M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3969201474009646 Test RMSE: 0.63001597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqingke\u001b[0m (\u001b[33mkeylab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/junjdong/morgan-bert/wandb/run-20240901_102022-3fmtpgit</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/keylab/huggingface/runs/3fmtpgit' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/keylab/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/keylab/huggingface' target=\"_blank\">https://wandb.ai/keylab/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/keylab/huggingface/runs/3fmtpgit' target=\"_blank\">https://wandb.ai/keylab/huggingface/runs/3fmtpgit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 20:22, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.710300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.334100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.304300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.280400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.268600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.246300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.244700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.233100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.203900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.202900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.193300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.189400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.184500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.181600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.174900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.171700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.167900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.164600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.164000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-10M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 20:36, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.648400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.343300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.316400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.289300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.206700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.196900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.185300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.178100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.175500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.174100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.170100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-10M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 19:00, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.648900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.340800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.305600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.292600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.260400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.245800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.217100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.204800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.196700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.195100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.182200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.180200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.174300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.171500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.167700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-10M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 17:22, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.336800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.308400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.285400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.256100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.243600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.180300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.178100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.172800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.167400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.164100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-10M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 18:21, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.321300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.295100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.242900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.219500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.205200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.198500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.192200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.189800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.184800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.178300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.174800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.166100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.161100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.158200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 18:18, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.322600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.289500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.266300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.240700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.238600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.226100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.217700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.213200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.201500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.191300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.191300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.190100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 18:11, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.367900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.339900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.315200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.302700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.282300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.277800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.260900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.259900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.250100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.236600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.220600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.217300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.210300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.206700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.203800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.200500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 17:17, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.359500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.328100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.277800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.245400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.240200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.232300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.201400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.198800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.196000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 17:18, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.292200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.263100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.258800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.234100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.229500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.226200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.220300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.203300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.198300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.192100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./baseline_models/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 17:54, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.335900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.282200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.217100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.207100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.198800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.188200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.181900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_1_s_0_atomFirst_f_300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 1:49:48, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.387400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.245100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.191400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.151800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.104800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.090100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.065900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.033700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.014400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_1_s_0_atomFirst_f_300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 2:11:43, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.384400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.248800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.199700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.107200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.090600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.059700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.048100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.016400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_1_s_0_atomFirst_f_300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 2:14:13, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.253200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.193500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.160700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.105600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.059300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.041800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_1_s_0_atomFirst_f_300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 1:20:50, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.382000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.249400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.191300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.149500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.088100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.041700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_1_s_0_atomFirst_f_300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 1:18:07, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.367800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.234300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.089100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.074400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.040700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.037100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_1_s_0_atomFirst_f_20 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 1:19:01, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.157200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.043500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.014600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_1_s_0_atomFirst_f_20 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3587' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3587/13620 20:35 < 57:36, 2.90 it/s, Epoch 7.90/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.390600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.193500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.155100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.109300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.092900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda3/envs/junjie/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "ncats_mouse_ip_model_perf = utils_benchmark.run_benchmark(task_name, ncats_mouse_ip_model_perf, data_dir='./benchmarks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7e8598-31a1-4ad0-b4b4-279f7a95ca14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of compounds: 36267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_2_s_0_atomFirst_f_2300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 1:21:04, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.369300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.168300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.061400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.039300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.012600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.010900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_2_s_0_atomFirst_f_2300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 1:20:34, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.136100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.085600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.035500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.031600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.015700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.014900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.011400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_2_s_0_atomFirst_f_2300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 1:20:59, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.396400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.173500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.131400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.084900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.059300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.025400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_2_s_0_atomFirst_f_2300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 1:20:35, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.398800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.087800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.060600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.011100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_2_s_0_atomFirst_f_2300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 1:22:43, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.371700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.103500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_2_s_0_radiusFirst_f_2300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 1:23:56, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.359400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.222100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.129200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.086500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.040400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.030800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_2_s_0_radiusFirst_f_2300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 1:20:09, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.384600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.229900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.131700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.087100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.022400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.012600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_2_s_0_radiusFirst_f_2300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 1:20:36, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.367600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.169100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.088100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.012700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_2_s_0_radiusFirst_f_2300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 1:20:17, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.377600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.172200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.132100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.107300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.088400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.074900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.055800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.040300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./MorganBERT_models/MorganBERT_base_full_r_2_s_0_radiusFirst_f_2300 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 1:20:41, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.347300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.207300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.101300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.069300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.009400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:34: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 0 which\n",
      "    has less than 75% of the memory or cores of GPU 6. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'RF+MACCS'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'RF+PubChemFP'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'RF+ECFP2'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'RF+ECFP4'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'RF+Daylight'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'RF+RDKitFP'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'D-MPNN'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'ChemBERTa-10M-MLM'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/junjdong/miniconda3/envs/morgan/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'ChemBERTa-77M-MLM'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncats_mouse_ip_model_perf = utils_benchmark.run_benchmark(task_name, ncats_mouse_ip_model_perf, data_dir='./benchmarks/')\n",
    "len(ncats_mouse_ip_model_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1d56a2-9ed0-4cba-8de8-0e32776e627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of compounds: 36267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RF+Mol2vec: 100%|| 5/5 [08:35<00:00, 103.03s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncats_mouse_ip_model_perf = utils_benchmark.run_benchmark(task_name, ncats_mouse_ip_model_perf)\n",
    "len(ncats_mouse_ip_model_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "693cd03a-22e0-4af5-a785-cc3a8b8e615f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- eval_mse --------------------\n",
      "KNN 0.323 $\\pm$ 0.068\n",
      "RF+MACCS 0.312 $\\pm$ 0.071\n",
      "RF+PubChemFP 0.318 $\\pm$ 0.076\n",
      "RF+ECFP2 0.308 $\\pm$ 0.064\n",
      "RF+ECFP4 0.316 $\\pm$ 0.066\n",
      "RF+Daylight 0.347 $\\pm$ 0.076\n",
      "RF+RDKitFP 0.331 $\\pm$ 0.079\n",
      "RF+Mol2vec 0.316 $\\pm$ 0.063\n",
      "D-MPNN 0.300 $\\pm$ 0.090\n",
      "MolCLR 0.314 $\\pm$ 0.049\n",
      "ChemBERTa-10M-MLM 0.308 $\\pm$ 0.060\n",
      "ChemBERTa-77M-MLM 0.322 $\\pm$ 0.057\n",
      "MolFormer 0.303 $\\pm$ 0.058\n",
      "MorganBERT_base_full_r_0_s_0 0.291 $\\pm$ 0.047\n",
      "MorganBERT_base_full_r_1_s_0_atomFirst_f_300 0.278 $\\pm$ 0.031\n",
      "MorganBERT_base_full_r_1_s_0_radiusFirst_f_300 0.282 $\\pm$ 0.036\n",
      "MorganBERT_base_full_r_2_s_0_atomFirst_f_2300 0.285 $\\pm$ 0.050\n",
      "MorganBERT_base_full_r_2_s_0_radiusFirst_f_2300 0.276 $\\pm$ 0.037\n",
      "\n",
      "\n",
      "-------------------- eval_mae --------------------\n",
      "KNN 0.388 $\\pm$ 0.041\n",
      "RF+MACCS 0.386 $\\pm$ 0.045\n",
      "RF+PubChemFP 0.386 $\\pm$ 0.041\n",
      "RF+ECFP2 0.379 $\\pm$ 0.037\n",
      "RF+ECFP4 0.383 $\\pm$ 0.039\n",
      "RF+Daylight 0.417 $\\pm$ 0.041\n",
      "RF+RDKitFP 0.401 $\\pm$ 0.040\n",
      "RF+Mol2vec 0.397 $\\pm$ 0.041\n",
      "D-MPNN 0.374 $\\pm$ 0.045\n",
      "MolCLR 0.388 $\\pm$ 0.035\n",
      "ChemBERTa-10M-MLM 0.384 $\\pm$ 0.038\n",
      "ChemBERTa-77M-MLM 0.396 $\\pm$ 0.035\n",
      "MolFormer 0.385 $\\pm$ 0.033\n",
      "MorganBERT_base_full_r_0_s_0 0.375 $\\pm$ 0.028\n",
      "MorganBERT_base_full_r_1_s_0_atomFirst_f_300 0.371 $\\pm$ 0.021\n",
      "MorganBERT_base_full_r_1_s_0_radiusFirst_f_300 0.376 $\\pm$ 0.025\n",
      "MorganBERT_base_full_r_2_s_0_atomFirst_f_2300 0.374 $\\pm$ 0.031\n",
      "MorganBERT_base_full_r_2_s_0_radiusFirst_f_2300 0.372 $\\pm$ 0.026\n",
      "\n",
      "\n",
      "-------------------- eval_r2 --------------------\n",
      "KNN 0.344 $\\pm$ 0.064\n",
      "RF+MACCS 0.368 $\\pm$ 0.065\n",
      "RF+PubChemFP 0.355 $\\pm$ 0.075\n",
      "RF+ECFP2 0.373 $\\pm$ 0.054\n",
      "RF+ECFP4 0.358 $\\pm$ 0.058\n",
      "RF+Daylight 0.296 $\\pm$ 0.075\n",
      "RF+RDKitFP 0.329 $\\pm$ 0.078\n",
      "RF+Mol2vec 0.358 $\\pm$ 0.054\n",
      "D-MPNN 0.394 $\\pm$ 0.104\n",
      "MolCLR 0.359 $\\pm$ 0.030\n",
      "ChemBERTa-10M-MLM 0.373 $\\pm$ 0.047\n",
      "ChemBERTa-77M-MLM 0.343 $\\pm$ 0.039\n",
      "MolFormer 0.101 $\\pm$ 0.159\n",
      "MorganBERT_base_full_r_0_s_0 0.405 $\\pm$ 0.024\n",
      "MorganBERT_base_full_r_1_s_0_atomFirst_f_300 0.431 $\\pm$ 0.012\n",
      "MorganBERT_base_full_r_1_s_0_radiusFirst_f_300 0.424 $\\pm$ 0.022\n",
      "MorganBERT_base_full_r_2_s_0_atomFirst_f_2300 0.419 $\\pm$ 0.033\n",
      "MorganBERT_base_full_r_2_s_0_radiusFirst_f_2300 0.436 $\\pm$ 0.022\n",
      "\n",
      "\n",
      "-------------------- eval_rmse --------------------\n",
      "KNN 0.566 $\\pm$ 0.057\n",
      "RF+MACCS 0.556 $\\pm$ 0.060\n",
      "RF+PubChemFP 0.561 $\\pm$ 0.064\n",
      "RF+ECFP2 0.553 $\\pm$ 0.055\n",
      "RF+ECFP4 0.560 $\\pm$ 0.056\n",
      "RF+Daylight 0.586 $\\pm$ 0.062\n",
      "RF+RDKitFP 0.572 $\\pm$ 0.065\n",
      "RF+Mol2vec 0.560 $\\pm$ 0.054\n",
      "D-MPNN 0.544 $\\pm$ 0.076\n",
      "MolCLR 0.559 $\\pm$ 0.043\n",
      "ChemBERTa-10M-MLM 0.553 $\\pm$ 0.052\n",
      "ChemBERTa-77M-MLM 0.566 $\\pm$ 0.049\n",
      "MolFormer 0.548 $\\pm$ 0.050\n",
      "MorganBERT_base_full_r_0_s_0 0.539 $\\pm$ 0.042\n",
      "MorganBERT_base_full_r_1_s_0_atomFirst_f_300 0.526 $\\pm$ 0.029\n",
      "MorganBERT_base_full_r_1_s_0_radiusFirst_f_300 0.530 $\\pm$ 0.034\n",
      "MorganBERT_base_full_r_2_s_0_atomFirst_f_2300 0.532 $\\pm$ 0.045\n",
      "MorganBERT_base_full_r_2_s_0_radiusFirst_f_2300 0.524 $\\pm$ 0.034\n",
      "\n",
      "\n",
      "-------------------- eval_pcc --------------------\n",
      "KNN 0.610 $\\pm$ 0.051\n",
      "RF+MACCS 0.605 $\\pm$ 0.057\n",
      "RF+PubChemFP 0.596 $\\pm$ 0.062\n",
      "RF+ECFP2 0.611 $\\pm$ 0.045\n",
      "RF+ECFP4 0.599 $\\pm$ 0.049\n",
      "RF+Daylight 0.550 $\\pm$ 0.073\n",
      "RF+RDKitFP 0.577 $\\pm$ 0.072\n",
      "RF+Mol2vec 0.603 $\\pm$ 0.045\n",
      "D-MPNN 0.662 $\\pm$ 0.040\n",
      "MolCLR 0.614 $\\pm$ 0.013\n",
      "ChemBERTa-10M-MLM 0.627 $\\pm$ 0.036\n",
      "ChemBERTa-77M-MLM 0.610 $\\pm$ 0.030\n",
      "MolFormer 0.654 $\\pm$ 0.041\n",
      "MorganBERT_base_full_r_0_s_0 0.658 $\\pm$ 0.025\n",
      "MorganBERT_base_full_r_1_s_0_atomFirst_f_300 0.679 $\\pm$ 0.010\n",
      "MorganBERT_base_full_r_1_s_0_radiusFirst_f_300 0.685 $\\pm$ 0.016\n",
      "MorganBERT_base_full_r_2_s_0_atomFirst_f_2300 0.666 $\\pm$ 0.027\n",
      "MorganBERT_base_full_r_2_s_0_radiusFirst_f_2300 0.686 $\\pm$ 0.018\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utils_benchmark.print_perf_table(ncats_mouse_ip_model_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58993afd-65d9-4999-a35d-432abf977167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
